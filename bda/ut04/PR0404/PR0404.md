------------- ESPECIALIZACIÓN EN INTELIGENCIA ARTIFICIAL Y BIG DATA -------------
---------------------------------------------------------------------------------

Módulo:                     BIG DATA APLICADO

Profesor:                   Víctor J. González

Unidad de Trabajo:          UT04. Procesamiento distribuido con MapReduce

Práctica:                   PR0404. Aplicación de patrones MapReduce

Resultados de aprendizaje:  RA1

## PR0404: Aplicación de patrones MapReduce

### Descripción

En esta práctica procesaréis el dataset de PIB mundial disponible en Kaggle y practicaremos en el uso de diferentes patrones de diseño de MapReduce.

### Dataset

El dataset tiene dos archivos, utilizaremos el llamado countries_gdp_hist.csv, que tiene los siguientes campos:

country_code: código de país

region_name: nombre de la región

sub_region_name: nombre de la subregión

intermediate_region: región intermedia

country_name: nombre de país

income_group: grupo de ingresos

year: año

total_gdp: Producto Interior Bruto total

total_gdp_million: Producto Interior Bruto en millones

gdp_variation: variación del Producto Interior Bruto


```python
!hdfs dfs -put /media/notebooks/countries* /
```


```python
!hdfs dfs -ls / |grep countri*
```

    -rw-r--r--   3 root supergroup    1658937 2025-12-18 09:08 /countries_gdp_hist.csv


### Ejercicio 1: Limpieza y Transformación

#### Patrón

Filtrado y transformación.

Objetivo

El dataset tiene años muy antiguos y valores nulos. Queremos un dataset limpio para años del siglo XXI.

Implementación

**Mapper**:

Lee línea por línea desde sys.stdin.

Valida: ignora cabeceras o líneas con errores de formato.

Filtra: conserva solo registros donde year >= 2000 y total_gdp > 0.

Transforma: emite solo Country Name, Year y Total GDP.

Salida: imprime en STDOUT separado por tabuladores (\t).

**Reducer**:

En este caso el reducer no tiene que hacer nada


```python
%%writefile mapper_ejercicio_pr404_1.py
#!/usr/bin/env python3
import os
import sys

for line in sys.stdin:
    line = line.strip()
     # line =  	country_code	region_name	sub_region_name	intermediate_region	country_name	income_group	year	total_gdp	total_gdp_million	gdp_variation
    lista = line.split(";")
    if len(lista) == 10: #Miramos si tiene exactamente 10 campos
        country_code,region_name,sub_region_name,intermediate_region,country_name,income_group,year,total_gdp,total_gdp_million,gdp_variation = lista
        if country_code == "country_code": #Comprobamos si es la cabecera
            continue
        else:   # entra sin cabecera
            country_code,region_name,sub_region_name,intermediate_region,country_name,income_group,year,total_gdp,total_gdp_million,gdp_variation = lista
            year = int(year)
            total_gdp = float(total_gdp)
            if year >= 2000 and total_gdp > 0: #Miramos que el año += 2000 y gpd > 0
                print(f"{country_name}\t{year}\t{total_gdp}")
            
    else: # no tiene 10 valores
        continue
        
```

    Overwriting mapper_ejercicio_pr404_1.py



```python
!cat /media/notebooks/countries_gdp_hist.csv |python3 /media/notebooks/mapper_ejercicio_pr404_1.py
```


```python
!hdfs dfs -ls /
```

    Found 32 items
    drwxr-xr-x   - root supergroup          0 2025-11-26 16:43 /Quijote
    drwxr-xr-x   - root supergroup          0 2025-11-26 18:18 /Quijote2
    drwxr-xr-x   - root supergroup          0 2025-11-27 18:03 /Quijote3
    drwxr-xr-x   - root supergroup          0 2025-11-27 18:06 /Quijote3_2
    drwxr-xr-x   - root supergroup          0 2025-11-24 11:29 /Temperatura_30_ciudad
    drwxr-xr-x   - root supergroup          0 2025-11-24 11:45 /Temperatura_MinMax
    drwxr-xr-x   - root supergroup          0 2025-11-24 11:18 /Temperatura_maxima
    drwxr-xr-x   - root supergroup          0 2025-11-24 11:19 /Temperatura_media_pais
    drwxr-xr-x   - root supergroup          0 2025-12-03 17:20 /angel
    drwxr-xr-x   - root supergroup          0 2025-12-03 17:52 /backup
    -rw-r--r--   3 root supergroup    6622610 2025-12-04 09:31 /clean_file.csv
    -rw-r--r--   3 root supergroup    1658937 2025-12-18 09:08 /countries_gdp_hist.csv
    drwxr-xr-x   - root supergroup          0 2025-12-03 19:29 /dia_antes
    drwxr-xr-x   - root supergroup          0 2025-12-04 08:55 /examen
    drwxr-xr-x   - root supergroup          0 2025-11-18 09:05 /indice_invertido
    drwxr-xr-x   - root supergroup          0 2025-12-16 15:42 /log
    drwxr-xr-x   - root supergroup          0 2025-12-16 15:47 /log1
    drwxr-xr-x   - root supergroup          0 2025-12-16 15:52 /log2
    drwxr-xr-x   - root supergroup          0 2025-12-16 15:57 /log3
    drwxr-xr-x   - root supergroup          0 2025-12-16 16:00 /log4
    drwxr-xr-x   - root supergroup          0 2025-12-16 16:07 /log5
    drwxr-xr-x   - root supergroup          0 2025-12-16 16:09 /log6
    drwxr-xr-x   - root supergroup          0 2025-12-03 17:04 /practica
    drwxr-xr-x   - root supergroup          0 2025-11-26 15:30 /practica_401
    drwxr-xr-x   - root supergroup          0 2025-12-16 15:24 /practica_log
    drwxr-xr-x   - root supergroup          0 2025-12-01 11:45 /practicar_examen
    drwxr-xr-x   - root supergroup          0 2025-12-03 17:51 /proyectos
    drwxr-xr-x   - root supergroup          0 2025-12-03 17:51 /proyectoss
    drwxr-xr-x   - root supergroup          0 2025-12-01 13:20 /prueba_practicar_1
    drwxr-xr-x   - root supergroup          0 2025-11-18 09:22 /salida_indice
    drwxrwx---   - root supergroup          0 2025-11-12 11:33 /tmp
    drwxrwxrwt   - root root                0 2025-12-16 15:34 /yarn



```python
!hadoop jar /usr/local/hadoop/share/hadoop/tools/lib/hadoop-streaming-3.4.0.jar \
-files mapper_ejercicio_pr404_1.py \
-D mapreduce.job.reduces=0 \
-mapper "python3 mapper_ejercicio_pr404_1.py" \
-input /countries_gdp_hist.csv \
-output /pr404_1
```

    packageJobJar: [/tmp/hadoop-unjar7177784607921491333/] [] /tmp/streamjob2926397276852671116.jar tmpDir=null
    2025-12-18 09:50:29,513 INFO client.DefaultNoHARMFailoverProxyProvider: Connecting to ResourceManager at yarnmanager/172.19.0.6:8032
    2025-12-18 09:50:29,602 INFO client.DefaultNoHARMFailoverProxyProvider: Connecting to ResourceManager at yarnmanager/172.19.0.6:8032
    2025-12-18 09:50:29,855 INFO mapreduce.JobResourceUploader: Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/root/.staging/job_1766048298398_0001
    2025-12-18 09:50:30,410 INFO mapred.FileInputFormat: Total input files to process : 1
    2025-12-18 09:50:30,544 INFO mapreduce.JobSubmitter: number of splits:2
    2025-12-18 09:50:30,743 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1766048298398_0001
    2025-12-18 09:50:30,744 INFO mapreduce.JobSubmitter: Executing with tokens: []
    2025-12-18 09:50:30,882 INFO conf.Configuration: resource-types.xml not found
    2025-12-18 09:50:30,882 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.
    2025-12-18 09:50:31,311 INFO impl.YarnClientImpl: Submitted application application_1766048298398_0001
    2025-12-18 09:50:31,361 INFO mapreduce.Job: The url to track the job: http://yarnmanager:8088/proxy/application_1766048298398_0001/
    2025-12-18 09:50:31,363 INFO mapreduce.Job: Running job: job_1766048298398_0001
    2025-12-18 09:50:36,471 INFO mapreduce.Job: Job job_1766048298398_0001 running in uber mode : false
    2025-12-18 09:50:36,472 INFO mapreduce.Job:  map 0% reduce 0%
    2025-12-18 09:50:41,551 INFO mapreduce.Job:  map 50% reduce 0%
    2025-12-18 09:50:42,559 INFO mapreduce.Job:  map 100% reduce 0%
    2025-12-18 09:50:43,582 INFO mapreduce.Job: Job job_1766048298398_0001 completed successfully
    2025-12-18 09:50:43,648 INFO mapreduce.Job: Counters: 33
    	File System Counters
    		FILE: Number of bytes read=0
    		FILE: Number of bytes written=627100
    		FILE: Number of read operations=0
    		FILE: Number of large read operations=0
    		FILE: Number of write operations=0
    		HDFS: Number of bytes read=1663223
    		HDFS: Number of bytes written=162107
    		HDFS: Number of read operations=14
    		HDFS: Number of large read operations=0
    		HDFS: Number of write operations=4
    		HDFS: Number of bytes read erasure-coded=0
    	Job Counters 
    		Launched map tasks=2
    		Data-local map tasks=2
    		Total time spent by all maps in occupied slots (ms)=6113
    		Total time spent by all reduces in occupied slots (ms)=0
    		Total time spent by all map tasks (ms)=6113
    		Total vcore-milliseconds taken by all map tasks=6113
    		Total megabyte-milliseconds taken by all map tasks=6259712
    	Map-Reduce Framework
    		Map input records=13761
    		Map output records=4983
    		Input split bytes=190
    		Spilled Records=0
    		Failed Shuffles=0
    		Merged Map outputs=0
    		GC time elapsed (ms)=107
    		CPU time spent (ms)=1530
    		Physical memory (bytes) snapshot=501071872
    		Virtual memory (bytes) snapshot=5233201152
    		Total committed heap usage (bytes)=558366720
    		Peak Map Physical memory (bytes)=266387456
    		Peak Map Virtual memory (bytes)=2617065472
    	File Input Format Counters 
    		Bytes Read=1663033
    	File Output Format Counters 
    		Bytes Written=162107
    2025-12-18 09:50:43,648 INFO streaming.StreamJob: Output directory: /pr404_1



```python
!hdfs dfs -head /pr404_1/part-00000
```

    LAO PEOPLE'S DEMOCRATIC REPUBLIC	2021	18827148530.9347
    LAO PEOPLE'S DEMOCRATIC REPUBLIC	2022	15468785203.7532
    LAO PEOPLE'S DEMOCRATIC REPUBLIC	2023	15843155731.2552
    LEBANON	2000	17260364842.4544
    LEBANON	2001	17649751243.7811
    LEBANON	2002	19152238805.9701
    LEBANON	2003	20082918739.6352
    LEBANON	2004	21159827992.0398
    LEBANON	2005	21497336498.8391
    LEBANON	2006	22022709851.4096
    LEBANON	2007	24827355014.9254
    LEBANON	2008	29118916105.4726
    LEBANON	2009	35399582928.6899
    LEBANON	2010	38443907042.1227
    LEBANON	2011	39927125961.5257
    LEBANON	2012	44016799515.7546
    LEBANON	2013	46880103080.597
    LEBANON	2014	48095213746.6003
    LEBANON	2015	49929337836.8159
    LEBANON	2016	51147308774.1294
    LEBANON	2017	53027680685.9038
    LEBANON	2018	54901519155.5556
    LEBANON	2019	51605959131.2741
    LEBANON	2020	31712128253.7961
    LEBANON	2021	23131941556.7843
    LEBANON	2022	20992421948.8081
    LIBERIA	2000	874000000.0
    LIBERIA	2001	906000000.0
    LIBERIA	2002	927000000.0
    LIBERIA	2003	748000000.0
    LIBERIA	2004	897000000.0
    LIBERIA	2005	949000000.0
    LIBERIA	2006	1119000

### Ejercicio 2: Agregación por clave

## Patrón

Resumen numérico (promedio).

### Objetivo

Calcular el PIB promedio histórico por cada Región (Asia, Americas, Europe…).

Implementación

### Mapper:

Extrae region_name y total_gdp.

Emite: REGION \t GDP

### Reducer:

Hadoop envía los datos ordenados por clave, hay que ir sumando los valores y el número de ocurrencias de la clave que llevamos. Cuando cambie la clave se emite el promedio de la región y se resetean contadores.

Salida esperada: AMERICAS 650000.50, EUROPE 540000.20, etc.


```python
%%writefile mapper_ejercicio_pr404_2.py
#!/usr/bin/env python3
import os
import sys

for line in sys.stdin:
    line = line.strip()
     # line =  	country_code	region_name	sub_region_name	intermediate_region	country_name	income_group	year	total_gdp	total_gdp_million	gdp_variation
    country_code,region_name,sub_region_name,intermediate_region,country_name,income_group,year,total_gdp,total_gdp_million,gdp_variation = line.split(";")
    if country_code == "country_code": #Comprobamos si es la cabecera
        continue
    else:   # entra sin cabecera
        print(f"{region_name}\t{total_gdp}")
        
           
```

    Writing mapper_ejercicio_pr404_2.py



```python
!head /media/notebooks/countries_gdp_hist.csv |python3 /media/notebooks/mapper_ejercicio_pr404_2.py
```

    AMERICAS	0.0
    AMERICAS	0.0
    AMERICAS	0.0
    AMERICAS	0.0
    AMERICAS	0.0
    AMERICAS	0.0
    AMERICAS	0.0
    AMERICAS	0.0
    AMERICAS	0.0



```python
%%writefile reducer_ejercicio_pr404_2.py
#!/usr/bin/env python3
import os
import sys
region_aux = None
total_aux = None
contador = None
for line in sys.stdin:
    line = line.strip()
    # line =  {region_name}\t{total_gdp}
    region_name,total_gdp = line.split("\t")
    total_gdp = float(total_gdp)
    if region_aux == None:
        region_aux = region_name
        total_aux = total_gdp
        contador = 1
    if region_aux == region_name:
        total_aux += total_gdp
        contador += 1
    else:
        total_aux = total_aux/contador
        print(f"{region_aux}\t{total_aux}")
        contador = 1
        region_aux = region_name
        total_aux = total_gdp
if region_aux is not None:
    total_aux = total_aux/contador
    print(f"{region_aux}\t{total_aux}")
        
```

    Writing reducer_ejercicio_pr404_2.py



```python
!head /media/notebooks/countries_gdp_hist.csv |python3 /media/notebooks/mapper_ejercicio_pr404_2.py |sort|python3 /media/notebooks/reducer_ejercicio_pr404_2.py
```

    AMERICAS	0.0



```python
!hadoop jar /usr/local/hadoop/share/hadoop/tools/lib/hadoop-streaming-3.4.0.jar \
-files mapper_ejercicio_pr404_2.py,reducer_ejercicio_pr404_2.py \
-mapper "python3 mapper_ejercicio_pr404_2.py" \
-reducer "python3 reducer_ejercicio_pr404_2.py" \
-input /countries_gdp_hist.csv \
-output /pr404_2
```

    packageJobJar: [/tmp/hadoop-unjar6064359286126092912/] [] /tmp/streamjob6531783652630776970.jar tmpDir=null
    2025-12-18 16:28:39,984 INFO client.DefaultNoHARMFailoverProxyProvider: Connecting to ResourceManager at yarnmanager/172.19.0.6:8032
    2025-12-18 16:28:40,055 INFO client.DefaultNoHARMFailoverProxyProvider: Connecting to ResourceManager at yarnmanager/172.19.0.6:8032
    2025-12-18 16:28:40,193 INFO mapreduce.JobResourceUploader: Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/root/.staging/job_1766072929105_0002
    2025-12-18 16:28:40,436 INFO mapred.FileInputFormat: Total input files to process : 1
    2025-12-18 16:28:40,487 INFO mapreduce.JobSubmitter: number of splits:2
    2025-12-18 16:28:40,554 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1766072929105_0002
    2025-12-18 16:28:40,554 INFO mapreduce.JobSubmitter: Executing with tokens: []
    2025-12-18 16:28:40,671 INFO conf.Configuration: resource-types.xml not found
    2025-12-18 16:28:40,672 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.
    2025-12-18 16:28:40,729 INFO impl.YarnClientImpl: Submitted application application_1766072929105_0002
    2025-12-18 16:28:40,755 INFO mapreduce.Job: The url to track the job: http://yarnmanager:8088/proxy/application_1766072929105_0002/
    2025-12-18 16:28:40,756 INFO mapreduce.Job: Running job: job_1766072929105_0002
    2025-12-18 16:28:44,803 INFO mapreduce.Job: Job job_1766072929105_0002 running in uber mode : false
    2025-12-18 16:28:44,804 INFO mapreduce.Job:  map 0% reduce 0%
    2025-12-18 16:28:48,844 INFO mapreduce.Job:  map 100% reduce 0%
    2025-12-18 16:28:52,873 INFO mapreduce.Job:  map 100% reduce 100%
    2025-12-18 16:28:52,884 INFO mapreduce.Job: Job job_1766072929105_0002 completed successfully
    2025-12-18 16:28:52,931 INFO mapreduce.Job: Counters: 54
    	File System Counters
    		FILE: Number of bytes read=320611
    		FILE: Number of bytes written=1584048
    		FILE: Number of read operations=0
    		FILE: Number of large read operations=0
    		FILE: Number of write operations=0
    		HDFS: Number of bytes read=1663223
    		HDFS: Number of bytes written=128
    		HDFS: Number of read operations=11
    		HDFS: Number of large read operations=0
    		HDFS: Number of write operations=2
    		HDFS: Number of bytes read erasure-coded=0
    	Job Counters 
    		Launched map tasks=2
    		Launched reduce tasks=1
    		Data-local map tasks=2
    		Total time spent by all maps in occupied slots (ms)=3038
    		Total time spent by all reduces in occupied slots (ms)=1462
    		Total time spent by all map tasks (ms)=3038
    		Total time spent by all reduce tasks (ms)=1462
    		Total vcore-milliseconds taken by all map tasks=3038
    		Total vcore-milliseconds taken by all reduce tasks=1462
    		Total megabyte-milliseconds taken by all map tasks=3110912
    		Total megabyte-milliseconds taken by all reduce tasks=1497088
    	Map-Reduce Framework
    		Map input records=13761
    		Map output records=13760
    		Map output bytes=293085
    		Map output materialized bytes=320617
    		Input split bytes=190
    		Combine input records=0
    		Combine output records=0
    		Reduce input groups=5
    		Reduce shuffle bytes=320617
    		Reduce input records=13760
    		Reduce output records=5
    		Spilled Records=27520
    		Shuffled Maps =2
    		Failed Shuffles=0
    		Merged Map outputs=2
    		GC time elapsed (ms)=137
    		CPU time spent (ms)=2550
    		Physical memory (bytes) snapshot=887406592
    		Virtual memory (bytes) snapshot=7842832384
    		Total committed heap usage (bytes)=987758592
    		Peak Map Physical memory (bytes)=337272832
    		Peak Map Virtual memory (bytes)=2611511296
    		Peak Reduce Physical memory (bytes)=270860288
    		Peak Reduce Virtual memory (bytes)=2620796928
    	Shuffle Errors
    		BAD_ID=0
    		CONNECTION=0
    		IO_ERROR=0
    		WRONG_LENGTH=0
    		WRONG_MAP=0
    		WRONG_REDUCE=0
    	File Input Format Counters 
    		Bytes Read=1663033
    	File Output Format Counters 
    		Bytes Written=128
    2025-12-18 16:28:52,932 INFO streaming.StreamJob: Output directory: /pr404_2



```python
!hdfs dfs -cat /pr404_2/part-00000
```

    AFRICA	17414341708.45637
    AMERICAS	250120768754.7781
    ASIA	205913293760.71982
    EUROPE	213179096872.7242
    OCEANIA	32533011434.877434


## Ejercicio 3: Máximos por grupo (Filtering/Top-K)

### Patrón

Top-K per Group.


#### Objetivo

Encontrar el año de mayor variación de PIB (gdp_variation) para cada país.


### Implementación

##### Mapper:

Emite: Country_Name \t Year,Variation (Concatena año y variación en el valor para no perder el dato del año).

##### Reducer:

Para cada país, recorre todas las variaciones.


Mantén en memoria solo la variación más alta encontrada hasta el momento y su año asociado.

Al cambiar de país, emite: PAIS \t AÑO_RECORD (VARIACION)


```python
%%writefile mapper_ejercicio_pr404_3.py
#!/usr/bin/env python3
import os
import sys

for line in sys.stdin:
    line = line.strip()
     # line =  	country_code	region_name	sub_region_name	intermediate_region	country_name	income_group	year	total_gdp	total_gdp_million	gdp_variation
    country_code,region_name,sub_region_name,intermediate_region,country_name,income_group,year,total_gdp,total_gdp_million,gdp_variation = line.split(";")
    if country_code == "country_code": #Comprobamos si es la cabecera
        continue
    else:   # entra sin cabecera
        print(f"{country_name}\t{year},{gdp_variation}")
```

    Overwriting mapper_ejercicio_pr404_3.py



```python
!head /media/notebooks/countries_gdp_hist.csv |python3 /media/notebooks/mapper_ejercicio_pr404_3.py
```

    ARUBA	1960,0.0
    ARUBA	1961,0.0
    ARUBA	1962,0.0
    ARUBA	1963,0.0
    ARUBA	1964,0.0
    ARUBA	1965,0.0
    ARUBA	1966,0.0
    ARUBA	1967,0.0
    ARUBA	1968,0.0



```python
%%writefile reducer_ejercicio_pr404_3.py
#!/usr/bin/env python3
import os
import sys
variation_aux = None
country_aux = None
year_aux = None
for line in sys.stdin:
    line = line.strip()
    # line =  {country_name}\t{year},{gdp_variation}
    country_name, year_variation = line.split("\t")
    year,variation = year_variation.split(",")

    if country_aux == None: 
        country_aux == country_name
        year_aux = year
        variation_aux = variation
    if country_aux == country_name:
        if variation > variation_aux:
            variation_aux = variation
            year_aux = year
    else:
        print(f"{country_aux}\t{year_aux}({variation_aux})")
        country_aux = country_name
        year_aux = year
        variation_aux = variation
if country_aux is not None:
    if variation > variation_aux:
        variation_aux = variation
        
    print(f"{country_aux}\t{year_aux}({variation_aux})")
```

    Overwriting reducer_ejercicio_pr404_3.py



```python
!head /media/notebooks/countries_gdp_hist.csv |python3 /media/notebooks/mapper_ejercicio_pr404_3.py |sort|python3 reducer_ejercicio_pr404_3.py
```

    None	1960(0.0)
    ARUBA	1960(0.0)



```python
!hadoop jar /usr/local/hadoop/share/hadoop/tools/lib/hadoop-streaming-3.4.0.jar \
-files mapper_ejercicio_pr404_3.py,reducer_ejercicio_pr404_3.py \
-mapper "python3 mapper_ejercicio_pr404_3.py" \
-reducer "python3 reducer_ejercicio_pr404_3.py" \
-input /countries_gdp_hist.csv \
-output /pr404_3
```


```python
!hdfs dfs -head /pr404_3/part-00000
```

    None	1960(0.0)
    AFGHANISTAN	2003(8.83227780288267)
    ALBANIA	1989(9.83654897187228)
    ALGERIA	1967(9.45296256280155)
    AMERICAN SAMOA	2020(4.41176470588236)
    ANDORRA	2022(9.56461225648184)
    ANGOLA	2012(8.54210707584141)
    ANTIGUA AND BARBUDA	2022(9.51639564359461)
    ARGENTINA	1969(9.67952601078326)
    ARMENIA	2001(9.59999999927597)
    ARUBA	2022(8.51791810517545)
    AUSTRALIA	1970(7.17569474653776)
    AUSTRIA	1970(7.12287550213442)
    AZERBAIJAN	1998(9.99999961153691)
    BAHAMAS	1967(9.59811387222918)
    BAHRAIN	1974(8.47222224387403)
    BANGLADESH	1974(9.59195630039562)
    BARBADOS	1970(9.5094029913739)
    BELARUS	2006(9.99999472133979)
    BELGIUM	1964(6.95668473344055)
    BELIZE	1988(9.78848090834173)
    BENIN	1981(9.95423116708119)
    BERMUDA	2000(9.31715641086083)
    BHUTAN	1976(8.95465028355598)
    BOLIVIA (PLURINATIONAL STATE OF)	1968(8.52944308831111)
    BOSNIA AND HERZEGOVINA	1999(9.52157018578963)
    BOTSWANA	1999(9.66724070276106)
    BRAZIL	1968(9.79999999999998)
    BRUNEI DARUSSALAM	1977(9.93403484610029)
    BULGARIA	2021(7.78061481406978)
    BURKINA FASO	1982(9.5621966218117

## Ejercicio 4: Join (Reduce-Side Join)

### Patrón

Reduce-Side Join.

Objetivo

Unir el dataset de PIB con un dataset auxiliar de códigos de país para obtener el nombre completo en español (simulado).

##### Implementación
    
Datos Auxiliares: necesitas un fichero que relacione los códigos de los países con su respectivo nombre. Puedes utilizar este dataset disponible en Github.

#### Estrategia:
    
Subir ambos archivos (gdp.csv y codes.csv) a HDFS.


```python
!hdfs dfs -put /media/notebooks/all.csv /
```


```python
!hdfs dfs -ls / | grep all*
```

    -rw-r--r--   3 root supergroup      20730 2026-01-08 08:50 /all.csv


##### Mapper

Debe detectar qué archivo está leyendo. Una forma de hacerlo sería contando la cantidad de columnas del fichero. Ten cuidado, porque en un fichero el separador es el punto y coma, mientras que en el otro es la coma, así que lo que puedes hacer es:

Separo por comas, si obtengo X campos es el fichero codes.csv
Si no, separo por punto y coma, y compruebo que el número de campos concuerda con el fichero gdp.csv

Si es codes.csv: Emite CODIGO \t A_NombreEsp (Tag ‘A’, que usaré en el reducer para distinguir entre uno y otro).

Si es gdp.csv: Emite CODIGO \t B_PIB (Tag ‘B’).




```python
%%writefile mapper_ejercicio_pr404_4.py
#!/usr/bin/env python3
import sys

for line in sys.stdin:
    line = line.strip()

    
    lista = line.split(",")
    if len(lista) == 11:
        name, alpha_2, alpha_3, country_code, iso_3166_2, region, sub_region, intermediate_region, region_code, sub_region_code, intermediate_region_code = lista

        
        if name == "name":
            continue
        else:
           
            print(f"{alpha_3}\tA_{name}")

    else:
        
        lista = line.split(";")
        if len(lista) == 10:
            country_code, region_name, sub_region_name, intermediate_region, country_name, income_group, year, total_gdp, total_gdp_million, gdp_variation = lista

            
            if country_code == "country_code":
                continue
            else:
                print(f"{country_code}\tB_{total_gdp}")

```

    Overwriting mapper_ejercicio_pr404_4.py



```python
!head /media/notebooks/countries_gdp_hist.csv |python3 /media/notebooks/mapper_ejercicio_pr404_4.py
```

    ABW	B_0.0
    ABW	B_0.0
    ABW	B_0.0
    ABW	B_0.0
    ABW	B_0.0
    ABW	B_0.0
    ABW	B_0.0
    ABW	B_0.0
    ABW	B_0.0



```python
!head /media/notebooks/all.csv |python3 /media/notebooks/mapper_ejercicio_pr404_4.py
```

    AFG	A_Afghanistan
    ALA	A_Åland Islands
    ALB	A_Albania
    DZA	A_Algeria
    ASM	A_American Samoa
    AND	A_Andorra
    AGO	A_Angola
    AIA	A_Anguilla
    ATA	A_Antarctica


##### Reducer

Recibirá todas las líneas de un mismo código juntas (ej. ESP).

Guarda el nombre en español (Tag A) en una variable.

Cuando lleguen los datos del PIB (Tag B), imprime: Nombre_Español \t PIB


```python
%%writefile reducer_ejercicio_pr404_4.py
#!/usr/bin/env python3
import sys

current_code = None
nombre_pais = None

for line in sys.stdin:
    line = line.strip()
    code, value = line.split("\t", 1)

    
    if code != current_code:
        current_code = code
        nombre_pais = None

    
    if value.startswith("A_"):
        nombre_pais = value[2:]

    
    elif value.startswith("B_") and nombre_pais is not None:
        pib = value[2:]
        print(f"{nombre_pais}\t{pib}")

```

    Overwriting reducer_ejercicio_pr404_4.py


Comprobamos en local


```python
!cat /media/notebooks/countries_gdp_hist.csv /media/notebooks/all.csv \
| python3 /media/notebooks/mapper_ejercicio_pr404_4.py \
| sort \
| python3 /media/notebooks/reducer_ejercicio_pr404_4.py \
| head
```

    Aruba	0.0
    Aruba	0.0
    Aruba	0.0
    Aruba	0.0
    Aruba	0.0
    Aruba	0.0
    Aruba	0.0
    Aruba	0.0
    Aruba	0.0
    Aruba	0.0
    Traceback (most recent call last):
      File "/media/notebooks/reducer_ejercicio_pr404_4.py", line 23, in <module>
        print(f"{nombre_pais}\t{pib}")
    BrokenPipeError: [Errno 32] Broken pipe
    sort: write failed: 'standard output': Broken pipe
    sort: write error


Probamos en *Streaming*


```python
!hadoop jar /usr/local/hadoop/share/hadoop/tools/lib/hadoop-streaming-3.4.0.jar \
-files mapper_ejercicio_pr404_4.py,reducer_ejercicio_pr404_4.py \
-mapper "python3 mapper_ejercicio_pr404_4.py" \
-reducer "python3 reducer_ejercicio_pr404_4.py" \
-input /countries_gdp_hist.csv \
-input /all.csv \
-output /pr404_4

```

    packageJobJar: [/tmp/hadoop-unjar8844525764521642093/] [] /tmp/streamjob3269243517528034042.jar tmpDir=null
    2026-01-09 18:05:18,080 INFO client.DefaultNoHARMFailoverProxyProvider: Connecting to ResourceManager at yarnmanager/172.19.0.6:8032
    2026-01-09 18:05:21,487 INFO client.DefaultNoHARMFailoverProxyProvider: Connecting to ResourceManager at yarnmanager/172.19.0.6:8032
    2026-01-09 18:05:21,627 INFO mapreduce.JobResourceUploader: Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/root/.staging/job_1767980973431_0002
    2026-01-09 18:05:21,896 INFO mapred.FileInputFormat: Total input files to process : 2
    2026-01-09 18:05:21,951 INFO mapreduce.JobSubmitter: number of splits:3
    2026-01-09 18:05:22,022 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1767980973431_0002
    2026-01-09 18:05:22,022 INFO mapreduce.JobSubmitter: Executing with tokens: []
    2026-01-09 18:05:22,114 INFO conf.Configuration: resource-types.xml not found
    2026-01-09 18:05:22,114 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.
    2026-01-09 18:05:22,152 INFO impl.YarnClientImpl: Submitted application application_1767980973431_0002
    2026-01-09 18:05:22,172 INFO mapreduce.Job: The url to track the job: http://yarnmanager:8088/proxy/application_1767980973431_0002/
    2026-01-09 18:05:22,173 INFO mapreduce.Job: Running job: job_1767980973431_0002
    2026-01-09 18:05:26,218 INFO mapreduce.Job: Job job_1767980973431_0002 running in uber mode : false
    2026-01-09 18:05:26,218 INFO mapreduce.Job:  map 0% reduce 0%
    2026-01-09 18:05:29,260 INFO mapreduce.Job:  map 100% reduce 0%
    2026-01-09 18:05:33,287 INFO mapreduce.Job:  map 100% reduce 100%
    2026-01-09 18:05:33,296 INFO mapreduce.Job: Job job_1767980973431_0002 completed successfully
    2026-01-09 18:05:33,341 INFO mapreduce.Job: Counters: 54
    	File System Counters
    		FILE: Number of bytes read=310699
    		FILE: Number of bytes written=1878643
    		FILE: Number of read operations=0
    		FILE: Number of large read operations=0
    		FILE: Number of write operations=0
    		HDFS: Number of bytes read=1684033
    		HDFS: Number of bytes written=172389
    		HDFS: Number of read operations=14
    		HDFS: Number of large read operations=0
    		HDFS: Number of write operations=2
    		HDFS: Number of bytes read erasure-coded=0
    	Job Counters 
    		Launched map tasks=3
    		Launched reduce tasks=1
    		Data-local map tasks=3
    		Total time spent by all maps in occupied slots (ms)=4973
    		Total time spent by all reduces in occupied slots (ms)=1298
    		Total time spent by all map tasks (ms)=4973
    		Total time spent by all reduce tasks (ms)=1298
    		Total vcore-milliseconds taken by all map tasks=4973
    		Total vcore-milliseconds taken by all reduce tasks=1298
    		Total megabyte-milliseconds taken by all map tasks=5092352
    		Total megabyte-milliseconds taken by all reduce tasks=1329152
    	Map-Reduce Framework
    		Map input records=14011
    		Map output records=13995
    		Map output bytes=282703
    		Map output materialized bytes=310711
    		Input split bytes=270
    		Combine input records=0
    		Combine output records=0
    		Reduce input groups=246
    		Reduce shuffle bytes=310711
    		Reduce input records=13995
    		Reduce output records=6976
    		Spilled Records=27990
    		Shuffled Maps =3
    		Failed Shuffles=0
    		Merged Map outputs=3
    		GC time elapsed (ms)=201
    		CPU time spent (ms)=3160
    		Physical memory (bytes) snapshot=1170272256
    		Virtual memory (bytes) snapshot=10460966912
    		Total committed heap usage (bytes)=1308098560
    		Peak Map Physical memory (bytes)=338534400
    		Peak Map Virtual memory (bytes)=2613260288
    		Peak Reduce Physical memory (bytes)=270303232
    		Peak Reduce Virtual memory (bytes)=2622496768
    	Shuffle Errors
    		BAD_ID=0
    		CONNECTION=0
    		IO_ERROR=0
    		WRONG_LENGTH=0
    		WRONG_MAP=0
    		WRONG_REDUCE=0
    	File Input Format Counters 
    		Bytes Read=1683763
    	File Output Format Counters 
    		Bytes Written=172389
    2026-01-09 18:05:33,341 INFO streaming.StreamJob: Output directory: /pr404_4



```python
!hdfs dfs -head /pr404_4/part-00000
```

    Aruba	1873452513.96648
    Aruba	1961843575.41899
    Aruba	2044111731.84358
    Aruba	2254830726.25698
    Aruba	2360017318.43575
    Aruba	2469782681.56425
    Aruba	2677641340.78212
    Aruba	2843024581.00559
    Aruba	2553793296.08939
    Aruba	2453597206.70391
    Aruba	2637859217.87709
    Aruba	2615208379.88827
    Aruba	0.0
    Aruba	2727849720.67039
    Aruba	2790849720.67039
    Aruba	2962907262.56983
    Aruba	2983635195.53073
    Aruba	3092429050.27933
    Aruba	3276184357.5419
    Aruba	3395798882.68156
    Aruba	2481857122.8401
    Aruba	2929446578.25104
    Aruba	3279343543.60416
    Aruba	3648573136.15155
    Aruba	1896456983.24022
    Aruba	0.0
    Aruba	0.0
    Aruba	0.0
    Aruba	0.0
    Aruba	0.0
    Aruba	0.0
    Aruba	0.0
    Aruba	0.0
    Aruba	0.0
    Aruba	0.0
    Aruba	0.0
    Aruba	0.0
    Aruba	0.0
    Aruba	0.0
    Aruba	0.0
    Aruba	0.0
    Aruba	0.0
    Aruba	0.0
    Aruba	0.0
    Aruba	0.0
    Aruba	0.0
    Aruba	0.0
    Aruba	0.0
    Aruba	0.0
    Aruba	0.0
    Aruba	405586592.178771
    Aruba	487709497.206704
    Aruba	596648044.692737
    Aruba	695530726.256983
    Aruba	764804469.273743
    Aruba	872067039.106145
    Aruba	958659217.877095
    Aruba	1083240223.46369
    Aruba	1245810055.86592
    Aruba	1

## Ejercicio 5: Distribución de Riqueza (Binning Pattern)

Patrón

Binning (Categorización en cubos).

#### Objetivo

Clasificar los registros en rangos de riqueza definidos manualmente para generar un histograma. En lugar de agrupar por una columna existente (como Región), debes crear tu propia clave de agrupación basada en lógica de negocio.

Queremos saber cuántos registros de la historia corresponden a economías “Pequeñas”, “Medianas” y “Grandes” basándonos en el total_gdp_million.

Las reglas de negocio (bins) son:

Economía Pequeña: GDP < 10,000 Millones.

Economía Mediana: 10,000 <= GDP < 1,000,000 Millones.

Economía Grande: GDP >= 1,000,000 Millones.

#### Implementación

##### Mapper

Leer total_gdp_million.

Determinar la categoría según las reglas de negocio.

Salida: CATEGORIA \t 1 (Emitimos un 1 para contar).


```python
%%writefile mapper_ejercicio_pr404_5.py
#!/usr/bin/env python3
import sys

for line in sys.stdin:
    line = line.strip()
    fields = line.split(";")

    
    if len(fields) != 10:
        continue

    country_code, region_name, sub_region_name, intermediate_region, country_name, income_group, year, total_gdp, total_gdp_million, gdp_variation = fields

    if country_code == "country_code":
        continue

    gdp_million = float(total_gdp_million)
      
    if gdp_million < 10000:
        categoria = "Economia_Pequena"
    elif gdp_million < 1_000_000:
        categoria = "Economia_Mediana"
    else:
        categoria = "Economia_Grande"

    print(f"{categoria}\t1")

```

    Writing mapper_ejercicio_pr404_5.py



```python
!head /media/notebooks/countries_gdp_hist.csv |python3 /media/notebooks/mapper_ejercicio_pr404_5.py
```

    Economia_Pequena	1
    Economia_Pequena	1
    Economia_Pequena	1
    Economia_Pequena	1
    Economia_Pequena	1
    Economia_Pequena	1
    Economia_Pequena	1
    Economia_Pequena	1
    Economia_Pequena	1


##### Reducer

Suma simple de los “1” recibidos por cada categoría.

Salida esperada:

 - Economía Grande    250
 - Economía Mediana   1500
 - Economía Pequeña   3000


```python
%%writefile reducer_ejercicio_pr404_5.py
#!/usr/bin/env python3
import sys

current_key = None
current_sum = 0

for line in sys.stdin:
    key, value = line.strip().split("\t")
    value = int(value)

    if key != current_key:
        if current_key is not None:
            print(f"{current_key}\t{current_sum}")
        current_key = key
        current_sum = 0

    current_sum += value

if current_key is not None:
    print(f"{current_key}\t{current_sum}")

```

    Writing reducer_ejercicio_pr404_5.py



```python
!head /media/notebooks/countries_gdp_hist.csv |python3 /media/notebooks/mapper_ejercicio_pr404_5.py |sort |python3 /media/notebooks/reducer_ejercicio_pr404_5.py
```

    Economia_Pequena	9



```python
!hadoop jar /usr/local/hadoop/share/hadoop/tools/lib/hadoop-streaming-3.4.0.jar \
-files mapper_ejercicio_pr404_5.py,reducer_ejercicio_pr404_5.py \
-mapper "python3 mapper_ejercicio_pr404_5.py" \
-reducer "python3 reducer_ejercicio_pr404_5.py" \
-input /countries_gdp_hist.csv \
-output /pr404_5
```

    packageJobJar: [/tmp/hadoop-unjar8159332353272269218/] [] /tmp/streamjob3950764142590282709.jar tmpDir=null
    2026-01-09 18:16:01,008 INFO client.DefaultNoHARMFailoverProxyProvider: Connecting to ResourceManager at yarnmanager/172.19.0.6:8032
    2026-01-09 18:16:01,076 INFO client.DefaultNoHARMFailoverProxyProvider: Connecting to ResourceManager at yarnmanager/172.19.0.6:8032
    2026-01-09 18:16:01,212 INFO mapreduce.JobResourceUploader: Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/root/.staging/job_1767980973431_0003
    2026-01-09 18:16:01,473 INFO mapred.FileInputFormat: Total input files to process : 1
    2026-01-09 18:16:01,528 INFO mapreduce.JobSubmitter: number of splits:2
    2026-01-09 18:16:01,604 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1767980973431_0003
    2026-01-09 18:16:01,604 INFO mapreduce.JobSubmitter: Executing with tokens: []
    2026-01-09 18:16:01,715 INFO conf.Configuration: resource-types.xml not found
    2026-01-09 18:16:01,716 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.
    2026-01-09 18:16:01,766 INFO impl.YarnClientImpl: Submitted application application_1767980973431_0003
    2026-01-09 18:16:01,790 INFO mapreduce.Job: The url to track the job: http://yarnmanager:8088/proxy/application_1767980973431_0003/
    2026-01-09 18:16:01,790 INFO mapreduce.Job: Running job: job_1767980973431_0003
    2026-01-09 18:16:05,830 INFO mapreduce.Job: Job job_1767980973431_0003 running in uber mode : false
    2026-01-09 18:16:05,830 INFO mapreduce.Job:  map 0% reduce 0%
    2026-01-09 18:16:08,859 INFO mapreduce.Job:  map 100% reduce 0%
    2026-01-09 18:16:12,883 INFO mapreduce.Job:  map 100% reduce 100%
    2026-01-09 18:16:12,898 INFO mapreduce.Job: Job job_1767980973431_0003 completed successfully
    2026-01-09 18:16:12,950 INFO mapreduce.Job: Counters: 54
    	File System Counters
    		FILE: Number of bytes read=288543
    		FILE: Number of bytes written=1519912
    		FILE: Number of read operations=0
    		FILE: Number of large read operations=0
    		FILE: Number of write operations=0
    		HDFS: Number of bytes read=1663223
    		HDFS: Number of bytes written=64
    		HDFS: Number of read operations=11
    		HDFS: Number of large read operations=0
    		HDFS: Number of write operations=2
    		HDFS: Number of bytes read erasure-coded=0
    	Job Counters 
    		Launched map tasks=2
    		Launched reduce tasks=1
    		Data-local map tasks=2
    		Total time spent by all maps in occupied slots (ms)=2897
    		Total time spent by all reduces in occupied slots (ms)=1371
    		Total time spent by all map tasks (ms)=2897
    		Total time spent by all reduce tasks (ms)=1371
    		Total vcore-milliseconds taken by all map tasks=2897
    		Total vcore-milliseconds taken by all reduce tasks=1371
    		Total megabyte-milliseconds taken by all map tasks=2966528
    		Total megabyte-milliseconds taken by all reduce tasks=1403904
    	Map-Reduce Framework
    		Map input records=13761
    		Map output records=13760
    		Map output bytes=261017
    		Map output materialized bytes=288549
    		Input split bytes=190
    		Combine input records=0
    		Combine output records=0
    		Reduce input groups=3
    		Reduce shuffle bytes=288549
    		Reduce input records=13760
    		Reduce output records=3
    		Spilled Records=27520
    		Shuffled Maps =2
    		Failed Shuffles=0
    		Merged Map outputs=2
    		GC time elapsed (ms)=169
    		CPU time spent (ms)=2620
    		Physical memory (bytes) snapshot=890167296
    		Virtual memory (bytes) snapshot=7850242048
    		Total committed heap usage (bytes)=995098624
    		Peak Map Physical memory (bytes)=340520960
    		Peak Map Virtual memory (bytes)=2614534144
    		Peak Reduce Physical memory (bytes)=271187968
    		Peak Reduce Virtual memory (bytes)=2622734336
    	Shuffle Errors
    		BAD_ID=0
    		CONNECTION=0
    		IO_ERROR=0
    		WRONG_LENGTH=0
    		WRONG_MAP=0
    		WRONG_REDUCE=0
    	File Input Format Counters 
    		Bytes Read=1663033
    	File Output Format Counters 
    		Bytes Written=64
    2026-01-09 18:16:12,950 INFO streaming.StreamJob: Output directory: /pr404_5



```python
!hdfs dfs -cat /pr404_5/part-00000
```

    Economia_Grande	423
    Economia_Mediana	4777
    Economia_Pequena	8560


### Ejercicio 6: Índice invertido de países (Inverted Index Pattern)

Patrón

Inverted Index (con deduplicación).

Objetivo

Generar una lista de búsqueda rápida. Dado un nivel de ingresos (income_group), queremos obtener la lista de todos los países únicos que pertenecen a ese grupo.

El dataset es una serie temporal. El par (INGRESO ALTO, ESPAÑA) aparece unas 60 veces (una vez por cada año desde 1960). El reducer debe ser capaz de eliminar duplicados para no listar “España” 60 veces.

#### Implementación

##### Mapper

Leer income_group y country_name.

Salida: INCOME_GROUP \t COUNTRY_NAME


```python
%%writefile mapper_ejercicio_pr404_6.py
#!/usr/bin/env python3
import sys

for line in sys.stdin:
    line = line.strip()
    fields = line.split(";")

    if len(fields) != 10:
        continue
    country_code, region_name, sub_region_name, intermediate_region,country_name, income_group, year, total_gdp, total_gdp_million, gdp_variation = fields
    if country_code == "country_code":
        continue

    print(f"{income_group}\t{country_name}")

```

    Writing mapper_ejercicio_pr404_6.py



```python
!head /media/notebooks/countries_gdp_hist.csv |python3 /media/notebooks/mapper_ejercicio_pr404_6.py
```

    INGRESO ALTO	ARUBA
    INGRESO ALTO	ARUBA
    INGRESO ALTO	ARUBA
    INGRESO ALTO	ARUBA
    INGRESO ALTO	ARUBA
    INGRESO ALTO	ARUBA
    INGRESO ALTO	ARUBA
    INGRESO ALTO	ARUBA
    INGRESO ALTO	ARUBA


##### Reducer

Recibir la lista de países para un grupo.

Almacenar los países en una estructura que no admita duplicados (como un set de Python) mientras se itera sobre la misma clave.

Al cambiar de clave, unir el set en un string separado por comas.

##### Salida esperada:

 - INGRESO ALTO    ARUBA, ESPAÑA, FRANCIA, ...

 - INGRESO BAJO    AFGANISTÁN, ...


```python
%%writefile reducer_ejercicio_pr404_6.py
#!/usr/bin/env python3
import sys

current_group = None
countries = set()

for line in sys.stdin:
    line = line.strip()
    group, country = line.split("\t", 1)

    if group != current_group:
        if current_group is not None:
            print(f"{current_group}\t{', '.join(sorted(countries))}")

        current_group = group
        countries = set()
    countries.add(country)
if current_group is not None:
    print(f"{current_group}\t{', '.join(sorted(countries))}")
```

    Writing reducer_ejercicio_pr404_6.py



```python
!head /media/notebooks/countries_gdp_hist.csv |python3 /media/notebooks/mapper_ejercicio_pr404_6.py |sort |python3 /media/notebooks/reducer_ejercicio_pr404_6.py
```

    INGRESO ALTO	ARUBA



```python
!hadoop jar /usr/local/hadoop/share/hadoop/tools/lib/hadoop-streaming-3.4.0.jar \
-files mapper_ejercicio_pr404_6.py,reducer_ejercicio_pr404_6.py \
-mapper "python3 mapper_ejercicio_pr404_6.py" \
-reducer "python3 reducer_ejercicio_pr404_6.py" \
-input /countries_gdp_hist.csv \
-output /pr404_6
```

    packageJobJar: [/tmp/hadoop-unjar6727566431890059542/] [] /tmp/streamjob5178440912280733102.jar tmpDir=null
    2026-01-09 18:22:30,245 INFO client.DefaultNoHARMFailoverProxyProvider: Connecting to ResourceManager at yarnmanager/172.19.0.6:8032
    2026-01-09 18:22:30,317 INFO client.DefaultNoHARMFailoverProxyProvider: Connecting to ResourceManager at yarnmanager/172.19.0.6:8032
    2026-01-09 18:22:30,443 INFO mapreduce.JobResourceUploader: Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/root/.staging/job_1767980973431_0004
    2026-01-09 18:22:30,709 INFO mapred.FileInputFormat: Total input files to process : 1
    2026-01-09 18:22:30,762 INFO mapreduce.JobSubmitter: number of splits:2
    2026-01-09 18:22:30,839 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1767980973431_0004
    2026-01-09 18:22:30,839 INFO mapreduce.JobSubmitter: Executing with tokens: []
    2026-01-09 18:22:30,942 INFO conf.Configuration: resource-types.xml not found
    2026-01-09 18:22:30,942 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.
    2026-01-09 18:22:30,975 INFO impl.YarnClientImpl: Submitted application application_1767980973431_0004
    2026-01-09 18:22:30,993 INFO mapreduce.Job: The url to track the job: http://yarnmanager:8088/proxy/application_1767980973431_0004/
    2026-01-09 18:22:30,994 INFO mapreduce.Job: Running job: job_1767980973431_0004
    2026-01-09 18:22:38,368 INFO mapreduce.Job: Job job_1767980973431_0004 running in uber mode : false
    2026-01-09 18:22:38,368 INFO mapreduce.Job:  map 0% reduce 0%
    2026-01-09 18:22:41,414 INFO mapreduce.Job:  map 100% reduce 0%
    2026-01-09 18:22:45,438 INFO mapreduce.Job:  map 100% reduce 100%
    2026-01-09 18:22:45,443 INFO mapreduce.Job: Job job_1767980973431_0004 completed successfully
    2026-01-09 18:22:45,494 INFO mapreduce.Job: Counters: 54
    	File System Counters
    		FILE: Number of bytes read=470214
    		FILE: Number of bytes written=1883254
    		FILE: Number of read operations=0
    		FILE: Number of large read operations=0
    		FILE: Number of write operations=0
    		HDFS: Number of bytes read=1663223
    		HDFS: Number of bytes written=2756
    		HDFS: Number of read operations=11
    		HDFS: Number of large read operations=0
    		HDFS: Number of write operations=2
    		HDFS: Number of bytes read erasure-coded=0
    	Job Counters 
    		Launched map tasks=2
    		Launched reduce tasks=1
    		Data-local map tasks=2
    		Total time spent by all maps in occupied slots (ms)=2666
    		Total time spent by all reduces in occupied slots (ms)=1296
    		Total time spent by all map tasks (ms)=2666
    		Total time spent by all reduce tasks (ms)=1296
    		Total vcore-milliseconds taken by all map tasks=2666
    		Total vcore-milliseconds taken by all reduce tasks=1296
    		Total megabyte-milliseconds taken by all map tasks=2729984
    		Total megabyte-milliseconds taken by all reduce tasks=1327104
    	Map-Reduce Framework
    		Map input records=13761
    		Map output records=13760
    		Map output bytes=442688
    		Map output materialized bytes=470220
    		Input split bytes=190
    		Combine input records=0
    		Combine output records=0
    		Reduce input groups=5
    		Reduce shuffle bytes=470220
    		Reduce input records=13760
    		Reduce output records=5
    		Spilled Records=27520
    		Shuffled Maps =2
    		Failed Shuffles=0
    		Merged Map outputs=2
    		GC time elapsed (ms)=125
    		CPU time spent (ms)=2430
    		Physical memory (bytes) snapshot=836292608
    		Virtual memory (bytes) snapshot=7846518784
    		Total committed heap usage (bytes)=975699968
    		Peak Map Physical memory (bytes)=284168192
    		Peak Map Virtual memory (bytes)=2615513088
    		Peak Reduce Physical memory (bytes)=272830464
    		Peak Reduce Virtual memory (bytes)=2619162624
    	Shuffle Errors
    		BAD_ID=0
    		CONNECTION=0
    		IO_ERROR=0
    		WRONG_LENGTH=0
    		WRONG_MAP=0
    		WRONG_REDUCE=0
    	File Input Format Counters 
    		Bytes Read=1663033
    	File Output Format Counters 
    		Bytes Written=2756
    2026-01-09 18:22:45,494 INFO streaming.StreamJob: Output directory: /pr404_6



```python
!hdfs dfs -cat /pr404_6/part-00000
```

    INGRESO ALTO	AMERICAN SAMOA, ANDORRA, ANTIGUA AND BARBUDA, ARUBA, AUSTRALIA, AUSTRIA, BAHAMAS, BAHRAIN, BARBADOS, BELGIUM, BERMUDA, BRUNEI DARUSSALAM, BULGARIA, CANADA, CAYMAN ISLANDS, CHILE, CROATIA, CURAÇAO, CYPRUS, CZECHIA, DENMARK, ESTONIA, FAROE ISLANDS, FINLAND, FRANCE, FRENCH POLYNESIA, GERMANY, GIBRALTAR, GREECE, GREENLAND, GUAM, GUYANA, HONG KONG, HUNGARY, ICELAND, IRELAND, ISLE OF MAN, ISRAEL, ITALY, JAPAN, KOREA, REPUBLIC OF, KUWAIT, LATVIA, LIECHTENSTEIN, LITHUANIA, LUXEMBOURG, MACAO, MALTA, MONACO, NAURU, NETHERLANDS, NEW CALEDONIA, NEW ZEALAND, NORTHERN MARIANA ISLANDS, NORWAY, OMAN, PALAU, PANAMA, POLAND, PORTUGAL, PUERTO RICO, QATAR, ROMANIA, RUSSIAN FEDERATION, SAINT KITTS AND NEVIS, SAINT MARTIN (FRENCH PART), SAN MARINO, SAUDI ARABIA, SEYCHELLES, SINGAPORE, SINT MAARTEN (DUTCH PART), SLOVAKIA, SLOVENIA, SPAIN, SWEDEN, SWITZERLAND, TRINIDAD AND TOBAGO, TURKS AND CAICOS ISLANDS, UNITED ARAB EMIRATES, UNITED KINGDOM OF GREAT BRITAIN AND NORTHERN IRELAND, UNITED STATES OF AMERICA, URUGUAY, VIRGIN ISLANDS (BRITISH), VIRGIN ISLANDS (U.S.)
    INGRESO MEDIANO ALTO	ALBANIA, ALGERIA, ARGENTINA, ARMENIA, AZERBAIJAN, BELARUS, BELIZE, BOSNIA AND HERZEGOVINA, BOTSWANA, BRAZIL, CHINA, COLOMBIA, COSTA RICA, CUBA, DOMINICA, DOMINICAN REPUBLIC, ECUADOR, EL SALVADOR, EQUATORIAL GUINEA, FIJI, GABON, GEORGIA, GRENADA, GUATEMALA, INDONESIA, IRAN (ISLAMIC REPUBLIC OF), IRAQ, JAMAICA, KAZAKHSTAN, LIBYA, MALAYSIA, MALDIVES, MARSHALL ISLANDS, MAURITIUS, MEXICO, MOLDOVA, REPUBLIC OF, MONGOLIA, MONTENEGRO, NAMIBIA, NORTH MACEDONIA, PARAGUAY, PERU, SAINT LUCIA, SAINT VINCENT AND THE GRENADINES, SERBIA, SOUTH AFRICA, SURINAME, THAILAND, TONGA, TURKEY, TURKMENISTAN, TUVALU, UKRAINE
    NO CLASIFICADO	VENEZUELA (BOLIVARIAN REPUBLIC OF)
    PAÍSES DE INGRESO BAJO	AFGHANISTAN, BURKINA FASO, BURUNDI, CENTRAL AFRICAN REPUBLIC, CHAD, CONGO, DEMOCRATIC REPUBLIC OF THE, ERITREA, ETHIOPIA, GAMBIA, GUINEA-BISSAU, KOREA (DEMOCRATIC PEOPLE'S REPUBLIC OF), LIBERIA, MADAGASCAR, MALAWI, MALI, MOZAMBIQUE, NIGER, RWANDA, SIERRA LEONE, SOMALIA, SOUTH SUDAN, SUDAN, SYRIAN ARAB REPUBLIC, TOGO, UGANDA, YEMEN
    PAÍSES DE INGRESO MEDIANO BAJO	ANGOLA, BANGLADESH, BENIN, BHUTAN, BOLIVIA (PLURINATIONAL STATE OF), CABO VERDE, CAMBODIA, CAMEROON, COMOROS, CONGO, CÔTE D'IVOIRE, DJIBOUTI, EGYPT, ESWATINI, GHANA, GUINEA, HAITI, HONDURAS, INDIA, JORDAN, KENYA, KIRIBATI, KYRGYZSTAN, LAO PEOPLE'S DEMOCRATIC REPUBLIC, LEBANON, LESOTHO, MAURITANIA, MICRONESIA (FEDERATED STATES OF), MOROCCO, MYANMAR, NEPAL, NICARAGUA, NIGERIA, PAKISTAN, PALESTINE, STATE OF, PAPUA NEW GUINEA, PHILIPPINES, SAMOA, SAO TOME AND PRINCIPE, SENEGAL, SOLOMON ISLANDS, SRI LANKA, TAJIKISTAN, TANZANIA, UNITED REPUBLIC OF, TIMOR-LESTE, TUNISIA, UZBEKISTAN, VANUATU, VIET NAM, ZAMBIA, ZIMBABWE

