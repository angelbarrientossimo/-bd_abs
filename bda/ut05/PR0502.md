------------- ESPECIALIZACIÓN EN INTELIGENCIA ARTIFICIAL Y BIG DATA -------------
---------------------------------------------------------------------------------

Módulo:                     BIG DATA APLICADO

Profesor:                   Víctor J. González

Unidad de Trabajo:          UT05. Procesamiento distribuido con PySpark

Práctica:                   PR0502. Manipulación básica de dataframes

Resultados de aprendizaje:  RA1

### PR0502. Manipulación básica de dataframes

Vamos a empezar a realizar operaciones básicas sobre los datasets que cargamos en la anterior práctica.

##### Dataset 1: Datos para la predicción del rendimiento en cultivos

Dataset cultivos


```python
#Creamos la sesion y cogemos los datos
from pyspark.sql import SparkSession

try:
    spark = ( SparkSession.builder
                .appName("angel_spark")
                .master("spark://spark-master:7077")
                .getOrCreate()
            )
    print("SparkSession iniciada correctamente.")
except Exception as e:
    print("Error en la conexion")
    print(e)
from pyspark.sql.types import StructType, StructField, StringType, DoubleType, BooleanType,IntegerType, LongType
schema = StructType([
    StructField("Crop",StringType(),True),
    StructField("Region",StringType(),True),
    StructField("Soil_Type",StringType(),True),
    StructField("Soil_pH",DoubleType(),True),
    StructField("Rainfall_mm",DoubleType(),True),
    StructField("Temperature_C",DoubleType(),True),
    StructField("Humidity_pct",DoubleType(),True),
    StructField("Fertilizer_Used_kg",DoubleType(),True),
    StructField("Irrigation",StringType(),True),
    StructField("Pesticides_Used_kg",DoubleType(),True),
    StructField("Planting_Density",DoubleType(),True),
    StructField("Previous_Crop",StringType(),True),
    StructField("Yield_ton_per_ha",DoubleType(),True)
])
df = (spark.read
        .format("csv")
        .option("header","true")
        .schema(schema)
        .load("./crop_yield_dataset.csv")
     )
```

    SparkSession iniciada correctamente.


##### 1.- Selección de características

Crea un nuevo DataFrame llamado df_sel que contenga únicamente las columnas: Crop, Region, Temperature_C, Rainfall_mm, Irrigation y Yield_ton_per_ha.


```python
from pyspark.sql import functions as F
from pyspark.sql.functions import col, lit

df_sel = df.select(col("Crop"),col("Region"),col("Temperature_C"),col("Rainfall_mm"),col("Irrigation"),col("Yield_ton_per_ha"))
```


```python
df_sel.show(2)
```

    +------+--------+-------------+-----------+----------+----------------+
    |  Crop|  Region|Temperature_C|Rainfall_mm|Irrigation|Yield_ton_per_ha|
    +------+--------+-------------+-----------+----------+----------------+
    | Maize|Region_C|         19.7|     1485.4|      Drip|          101.48|
    |Barley|Region_D|         29.1|      399.4| Sprinkler|          127.39|
    +------+--------+-------------+-----------+----------+----------------+
    only showing top 2 rows
    


##### 2.- Normalización de nombres

Los nombres actuales son muy largos y técnicos (tienen unidades). Necesitamos estandarizarlos al español o simplificarlos. Usando el DataFrame df_sel del ejercicio anterior, cambia los siguientes nombres:

Temperature_C -> Temperatura

Rainfall_mm -> Lluvia

Yield_ton_per_ha -> Rendimiento

Guarda el resultado en df_renamed.


```python
df_renamed = df.select(
    col("Crop"), \
    col("Region"), \
    col("Temperature_C").alias("Temperatura"), \
    col("Rainfall_mm").alias("Lluvia"), \
    col("Irrigation"), \
    col("Yield_ton_per_ha").alias("Rendimiento"))
```


```python
df_renamed.show(2)
```

    +------+--------+-----------+------+----------+-----------+
    |  Crop|  Region|Temperatura|Lluvia|Irrigation|Rendimiento|
    +------+--------+-----------+------+----------+-----------+
    | Maize|Region_C|       19.7|1485.4|      Drip|     101.48|
    |Barley|Region_D|       29.1| 399.4| Sprinkler|     127.39|
    +------+--------+-----------+------+----------+-----------+
    only showing top 2 rows
    


##### 3.- Filtrado de datos (filter)

Supón que queremos centrarnos en cultivos de maíz que han crecido en regiones calurosas (más de 25 grados). Filtra df_renamed para mantener solo las filas donde:

El cultivo (Crop) sea igual a “Maize”.

La temperatura (Temperatura) sea mayor a 25.


```python
df_renamed.where ((col("Crop") == "Maize") & (col("Temperatura") > 25)).show(2)
```

    +-----+--------+-----------+------+----------+-----------+
    | Crop|  Region|Temperatura|Lluvia|Irrigation|Rendimiento|
    +-----+--------+-----------+------+----------+-----------+
    |Maize|Region_D|       26.4|1054.3|      Drip|     169.06|
    |Maize|Region_C|       32.4| 846.1|      None|      162.2|
    +-----+--------+-----------+------+----------+-----------+
    only showing top 2 rows
    


##### 4.- Encadenamiento

En Spark podemos encadenar varias funciones. Repite las órdenes anteriores encadenadas en una única sentencia


```python
#Encadenamiento es poner todo junto
from pyspark.sql.functions import col 

df = df.select(
    col("Crop"),
    col("Region"),
    col("Temperature_C").alias("Temperatura"),
    col("Rainfall_mm").alias("Lluvia"),
    col("Irrigation"),
    col("Yield_ton_per_ha").alias("Rendimiento")
).where(
    (col("Crop") == "Maize") & (col("Temperature_C") > 25)
)

df.show(2)

```

    +-----+--------+-----------+------+----------+-----------+
    | Crop|  Region|Temperatura|Lluvia|Irrigation|Rendimiento|
    +-----+--------+-----------+------+----------+-----------+
    |Maize|Region_D|       26.4|1054.3|      Drip|     169.06|
    |Maize|Region_C|       32.4| 846.1|      None|      162.2|
    +-----+--------+-----------+------+----------+-----------+
    only showing top 2 rows
    


##### Dataset 2: Lugares famosos del mundo

Dataset lugares famosos


```python
schema1 = StructType([
    StructField("Place_Name",StringType(),True),
    StructField("Country",StringType(),True),
    StructField("City",StringType(),True),
    StructField("Annual_Visitors_Millions",IntegerType(),True),
    StructField("Type",StringType(),True),
    StructField("UNESCO_World_Heritage",StringType(),True),
    StructField("Year_Built",IntegerType(),True),
    StructField("Entry_Fee_USD",IntegerType(),True),
    StructField("Best_Visit_Month",StringType(),True),
    StructField("Region",StringType(),True),
    StructField("Tourism_Revenue_Million_USD",IntegerType(),True),
    StructField("Average_Visit_Duration_Hours",DoubleType(),True),
    StructField("Famous_For",StringType(),True)
])
df = (spark.read
        .format("csv")
        .option("header","true")
        .schema(schema1)
        .load("./world_famous_places_2024.csv")
     )

```

##### 1.- Selección de datos críticos

El dataset tiene mucha información descriptiva que no necesitamos para el análisis cuantitativo (como Famous_For o Best_Visit_Month).

Crea un nuevo DataFrame llamado df_base seleccionando únicamente: Place_Name, Country, UNESCO_World_Heritage, Entry_Fee_USD y Annual_Visitors_Millions.


```python
df_base = df.select(
    col("Place_Name"), \
    col("Country"), \
    col("UNESCO_World_Heritage"), \
    col("Entry_Fee_USD"), \
    col("Annual_Visitors_Millions"))
df_base.show(3)
```

    +-------------+-------------+---------------------+-------------+------------------------+
    |   Place_Name|      Country|UNESCO_World_Heritage|Entry_Fee_USD|Annual_Visitors_Millions|
    +-------------+-------------+---------------------+-------------+------------------------+
    | Eiffel Tower|       France|                   No|           35|                       7|
    | Times Square|United States|                   No|            0|                      50|
    |Louvre Museum|       France|                  Yes|           22|                    NULL|
    +-------------+-------------+---------------------+-------------+------------------------+
    only showing top 3 rows
    


##### 2.- Traducción y simplificación

Las columnas tienen nombres en inglés y son demasiado largos para los reportes en español. Sobre el DataFrame df_base, renombra las columnas de la siguiente manera y guarda el resultado en df_es:

Place_Name -> Lugar

UNESCO_World_Heritage -> Es_UNESCO

Entry_Fee_USD -> Precio_Entrada

Annual_Visitors_Millions -> Visitantes_Millones


```python
df_es = df.select( \
    col("Place_Name").alias("Lugar"), \
    col("Country"), \
    col("UNESCO_World_Heritage").alias("Es_UNESCO"), \
    col("Entry_Fee_USD").alias("Precio_Entrada"), \
    col("Annual_Visitors_Millions").alias("Visitantes_Millones")
)
df_es.show(3)
```

    +-------------+-------------+---------+--------------+-------------------+
    |        Lugar|      Country|Es_UNESCO|Precio_Entrada|Visitantes_Millones|
    +-------------+-------------+---------+--------------+-------------------+
    | Eiffel Tower|       France|       No|            35|                  7|
    | Times Square|United States|       No|             0|                 50|
    |Louvre Museum|       France|      Yes|            22|               NULL|
    +-------------+-------------+---------+--------------+-------------------+
    only showing top 3 rows
    


##### 3: Filtrado

Supón que vamos a realizar una campaña y necesitamos filtrar los destinos que cumplan dos condiciones estrictas. Filtra df_es para obtener solo los registros que cumplan:

Sean Patrimonio de la Humanidad (Es_UNESCO es igual a “Yes”).

El precio de entrada (Precio_Entrada) sea menor o igual a 20 dólares.


```python
df_es.where((col("Es_UNESCO") == "Yes") & (col("Precio_Entrada")<= 20)).show(3)
```

    +-------------------+-------+---------+--------------+-------------------+
    |              Lugar|Country|Es_UNESCO|Precio_Entrada|Visitantes_Millones|
    +-------------------+-------+---------+--------------+-------------------+
    |Great Wall of China|  China|      Yes|            10|                 10|
    |          Taj Mahal|  India|      Yes|            15|               NULL|
    |          Colosseum|  Italy|      Yes|            18|               NULL|
    +-------------------+-------+---------+--------------+-------------------+
    only showing top 3 rows
    


##### Dataset 3: Registro turístico de Castilla y León

Dataset registro turístico




```python
schema = StructType([
    StructField("establecimiento",StringType(),True),
    StructField("n_registro",StringType(),True),
    StructField("codigo",StringType(),True),
    StructField("tipo",StringType(),True),
    StructField("categoria",StringType(),True),
    StructField("especialidades",StringType(),True),
    StructField("clase",StringType(),True),
    StructField("nombre",StringType(),True),
    StructField("direccion",StringType(),True),
    StructField("c_postal",IntegerType(),True),
    StructField("provincia",StringType(),True),
    StructField("municipio",StringType(),True),
    StructField("localidad",StringType(),True),
    StructField("nucleo",StringType(),True),
    StructField("telefono_1",LongType(),True),
    StructField("telefono_2",LongType(),True),
    StructField("telefono_3",LongType(),True),
    StructField("email",StringType(),True),
    StructField("web",StringType(),True),
    StructField("q_calidad",StringType(),True),
    StructField("posada_real",StringType(),True),
    StructField("plazas",IntegerType(),True),
    StructField("gps_longitud",DoubleType(),True),
    StructField("gps_latitud",DoubleType(),True),
    StructField("accesible_a_personas_con_discapacidad",StringType(),True),
    StructField("column_27",StringType(),True),
    StructField("posicion",StringType(),True),

])
df = (spark.read
        .format("csv")
        .option("header","true")
        .option("sep",";")
        .schema(schema)
        .load("registro-de-turismo-de-castilla-y-leon.csv")
     )
df.show(3)
```

    +--------------------+----------+------+--------------------+-----------+--------------+-----+--------------------+--------------------+--------+---------+---------+---------------+---------------+----------+----------+----------+--------------------+--------------------+---------+-----------+------+------------+-----------+-------------------------------------+---------+--------------------+
    |     establecimiento|n_registro|codigo|                tipo|  categoria|especialidades|clase|              nombre|           direccion|c_postal|provincia|municipio|      localidad|         nucleo|telefono_1|telefono_2|telefono_3|               email|                 web|q_calidad|posada_real|plazas|gps_longitud|gps_latitud|accesible_a_personas_con_discapacidad|column_27|            posicion|
    +--------------------+----------+------+--------------------+-----------+--------------+-----+--------------------+--------------------+--------+---------+---------+---------------+---------------+----------+----------+----------+--------------------+--------------------+---------+-----------+------+------------+-----------+-------------------------------------+---------+--------------------+
    |      Turismo Activo| 47/000047|  NULL|Profesional de Tu...|       NULL|          NULL| NULL|BERNARDO MORO MEN...|Calle Rio Somiedo...|   33840| Asturias|  Somiedo|POLA DE SOMIEDO|POLA DE SOMIEDO| 616367277|      NULL|      NULL|bernardomoro@hotm...|                NULL|     NULL|       NULL|  NULL|        NULL|       NULL|                                 NULL|     NULL|                NULL|
    |Alojam. Turismo R...| 05/000788|  NULL|Casa Rural de Alq...|3 Estrellas|          NULL| NULL|        LA SASTRERÍA|Calle VEINTIOCHO ...|    5296|    Ávila|  Adanero|        ADANERO|        ADANERO| 920307158| 606945069| 609289521|                NULL|www.lasastreriade...|     NULL|       NULL|     6|        NULL|       NULL|                                 NULL|     NULL|                NULL|
    |Alojam. Turismo R...| 05/000696|  NULL|Casa Rural de Alq...|4 Estrellas|          NULL| NULL|         LAS HAZANAS|       Plaza MAYOR 4|    5296|    Ávila|  Adanero|        ADANERO|        ADANERO| 655099974|      NULL|      NULL|lashazanas@hotmai...|                NULL|     NULL|       NULL|     8|  -4.6033331| 40.9438881|                                 NULL|     NULL|40.9438881, -4.60...|
    +--------------------+----------+------+--------------------+-----------+--------------+-----+--------------------+--------------------+--------+---------+---------+---------------+---------------+----------+----------+----------+--------------------+--------------------+---------+-----------+------+------------+-----------+-------------------------------------+---------+--------------------+
    only showing top 3 rows
    


    26/01/22 09:40:30 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.


##### 1.- Selección y saneamiento

Las columnas originales como N.Registro o GPS.Latitud tienen puntos, lo cual suele dar problemas en motores SQL o al guardar en Parquet. Además, solo necesitamos datos de contacto.

Crea un df_contactos seleccionando únicamente: Nombre, Tipo, Provincia, web y Email.




```python
df_contactos = df.select( \
    col("Nombre"), \
    col("Tipo"), \
    col("Provincia" ), \
    col("web"), \
    col("Email"))
df_contactos.show(20)
```

    +--------------------+--------------------+---------+--------------------+--------------------+
    |              Nombre|                Tipo|Provincia|                 web|               Email|
    +--------------------+--------------------+---------+--------------------+--------------------+
    |BERNARDO MORO MEN...|Profesional de Tu...| Asturias|                NULL|bernardomoro@hotm...|
    |        LA SASTRERÍA|Casa Rural de Alq...|    Ávila|www.lasastreriade...|                NULL|
    |         LAS HAZANAS|Casa Rural de Alq...|    Ávila|                NULL|lashazanas@hotmai...|
    | LA CASITA DEL PAJAR|Casa Rural de Alq...|    Ávila|                NULL|lashazanas@hotmai...|
    |            MARACANA|                 Bar|    Ávila|                NULL|emo123anatoliev@g...|
    |               PLAZA|                 Bar|    Ávila|                NULL|                NULL|
    |          LA OFICINA|                 Bar|    Ávila|                NULL|                NULL|
    |              JARDIN|                 Bar|    Ávila|                NULL|                NULL|
    |               CESAR|           Cafetería|    Ávila|                NULL|                NULL|
    |             ADANERO|           Cafetería|    Ávila|                NULL|                NULL|
    |                  MA|   Restaurante / Bar|    Ávila|                NULL|info@restaurantem...|
    |chalet encantador...|               Chalé|    Ávila|                NULL|correcaminos_ssp@...|
    |  FINCA AGUATACHALES|    Inmueble análogo|    Ávila|                NULL|fausto.saez.illob...|
    |JARDIN BOTÁNICO V...|p - Actividades d...|    Ávila|WWW.JARDINTIETAR.COM|AXEL.MAHLAU@GMAIL...|
    |P.INFORMACIÓN T. ...|n - Oficinas y pu...|    Ávila|                NULL|oficinaturismolaa...|
    |          LA ESPUELA|  Albergue turístico|    Ávila|www.turismorurall...|info@turismorural...|
    |           CONCEJO I|Casa Rural de Alq...|    Ávila|                NULL|                NULL|
    |       EL TIO MORENO|          Casa Rural|    Ávila|                NULL|   jgabari@gmail.com|
    |          CONCEJO II|Casa Rural de Alq...|    Ávila|                NULL|                NULL|
    |         LOS ABUELOS|Casa Rural de Alq...|    Ávila|                NULL|bonyros@telefonic...|
    +--------------------+--------------------+---------+--------------------+--------------------+
    only showing top 20 rows
    


##### 2.- Renombrado estándar

Vamos a renombrar las columnas para que estén en minúsculas y no tengan ambigüedades

Nombre nombre_establecimiento

Tipo categoria_actividad

web sitio_web

Email correo_electronico

Guarda el resultado en df_limpio.




```python
df_limpio = df_contactos.select( \
    col("Nombre").alias("nombre_establecimiento"), \
    col("Tipo").alias("categoria_actividad"),
    col("Provincia"), \
    col("web").alias("sitio_web"), \
    col("Email").alias("correo_electronico")
)
df_limpio.show(3)
```

    +----------------------+--------------------+---------+--------------------+--------------------+
    |nombre_establecimiento| categoria_actividad|Provincia|           sitio_web|  correo_electronico|
    +----------------------+--------------------+---------+--------------------+--------------------+
    |  BERNARDO MORO MEN...|Profesional de Tu...| Asturias|                NULL|bernardomoro@hotm...|
    |          LA SASTRERÍA|Casa Rural de Alq...|    Ávila|www.lasastreriade...|                NULL|
    |           LAS HAZANAS|Casa Rural de Alq...|    Ávila|                NULL|lashazanas@hotmai...|
    +----------------------+--------------------+---------+--------------------+--------------------+
    only showing top 3 rows
    


##### 3: Filtrado de texto

La columna categoria_actividad contiene valores sucios como “g - Bodegas y los complejos de enoturismo”. No podemos filtrar por igualdad exacta (==).

Filtra df_limpio para obtener una lista df_final que cumpla todas estas condiciones simultáneamente:

Ubicación: la Provincia debe ser “Burgos”.

Actividad: la categoria_actividad debe contener la palabra “Bodegas” (investiga contains] o like).

Digitalización: el sitio_web no puede estar vacío ni ser nulo (investiga la función isNotNull).


```python
df_final = df_limpio.where((col("Provincia")== "Burgos") & (col("categoria_actividad").like("%Bodegas%")) & col("sitio_web").isNotNull())
df_final.show(3)


```

    +----------------------+--------------------+---------+--------------------+--------------------+
    |nombre_establecimiento| categoria_actividad|Provincia|           sitio_web|  correo_electronico|
    +----------------------+--------------------+---------+--------------------+--------------------+
    |        BODEGAS TARSUS|g - Bodegas y los...|   Burgos|  www.tarsusvino.com|                NULL|
    |  BODEGAS DOMINIO D...|g - Bodegas y los...|   Burgos|www.dominiodecair...|bodegas@dominiode...|
    |    TERRITORIO LUTHIER|g - Bodegas y los...|   Burgos|territorioluthier...|luthier@territori...|
    +----------------------+--------------------+---------+--------------------+--------------------+
    only showing top 3 rows
    

