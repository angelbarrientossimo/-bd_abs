------------- ESPECIALIZACIÓN EN INTELIGENCIA ARTIFICIAL Y BIG DATA -------------
---------------------------------------------------------------------------------

Módulo:                     BIG DATA APLICADO

Profesor:                   Víctor J. González

Unidad de Trabajo:          UT05. Procesamiento distribuido con PySpark

Práctica:                   PR0503. Limpieza de datos sobre dataset de cultivos

Resultados de aprendizaje:  RA1

## PR0503. Limpieza de datos sobre dataset de cultivos

Seguimos avanzando en el conocimiento de PySpark realizando tareas más avanzadas.

#### Dataset 1: Datos para la predicción del rendimiento en cultivos

Dataset cultivos

Supón que queremos preparar los datos de nuestro dataset de cultivos para un modelo de redes neuronales y nos han pedido cuatro transformaciones específicas:

Generar identificadores únicos estandarizados

Normalizar las distribuciones numéricas

Comparar insumos

Proyectar fechas de cosecha




```python
#Creamos la sesion y cogemos los datos
from pyspark.sql import SparkSession

try:
    spark = ( SparkSession.builder
                .appName("angel_spark")
                .master("spark://spark-master:7077")
                .getOrCreate()
            )
    print("SparkSession iniciada correctamente.")
except Exception as e:
    print("Error en la conexion")
    print(e)
from pyspark.sql.types import StructType, StructField, StringType, DoubleType, BooleanType,IntegerType, LongType
schema = StructType([
    StructField("Crop",StringType(),True),
    StructField("Region",StringType(),True),
    StructField("Soil_Type",StringType(),True),
    StructField("Soil_pH",DoubleType(),True),
    StructField("Rainfall_mm",DoubleType(),True),
    StructField("Temperature_C",DoubleType(),True),
    StructField("Humidity_pct",DoubleType(),True),
    StructField("Fertilizer_Used_kg",DoubleType(),True),
    StructField("Irrigation",StringType(),True),
    StructField("Pesticides_Used_kg",DoubleType(),True),
    StructField("Planting_Density",DoubleType(),True),
    StructField("Previous_Crop",StringType(),True),
    StructField("Yield_ton_per_ha",DoubleType(),True)
])
df = (spark.read
        .format("csv")
        .option("header","true")
        .schema(schema)
        .load("./crop_yield_dataset.csv")
     )
```

    Setting default log level to "WARN".
    To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
    26/01/30 10:34:56 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable


    SparkSession iniciada correctamente.


#### 1.- Creación de un ID único

Necesitamos un código único para cada registro que sirva como clave primaria. Crea una nueva columna llamada Crop_ID en un nuevo DataFrame df_eng. Este ID debe seguir este formato estricto:

Ejemplo Final : **CODIGO_XXC_MAIZE**

CODIGO_REGION-CULTIVO.

Limpieza: de la columna Region (ej. “Region_C”), elimina la palabra “Region_” y quédate solo con la letra (puedes usar substring o split).

Formato: convierte el nombre del cultivo (Crop) a mayúsculas (upper).

Concatenación: une la letra de la región y el cultivo con un guion medio (concat_ws).

Relleno: si por algún motivo la letra de la región fuera muy corta (improbable aquí, pero por seguridad), asegúrate de que esa parte tenga al menos 3 caracteres rellenando con ‘X’ a la izquierda (lpad).

Nota: Como en este dataset es solo una letra, el lpad rellenará con dos X, ej: “XXC”.


```python
from pyspark.sql.functions import col, lit, split, upper, concat_ws, concat, lpad
(
    df
    .withColumn("Region", split(col("Region"), "_")[1])
    .withColumn("Crop", upper(col("Crop")))
    .withColumn("Concat_Region",concat_ws("-",concat(lit("CODIGO_"),lpad(col("Region"), 3, "X")),col("Crop")))
).show(9)

```

                                                                                    

    +------+------+---------+-------+-----------+-------------+------------+------------------+----------+------------------+----------------+-------------+----------------+-----------------+
    |  Crop|Region|Soil_Type|Soil_pH|Rainfall_mm|Temperature_C|Humidity_pct|Fertilizer_Used_kg|Irrigation|Pesticides_Used_kg|Planting_Density|Previous_Crop|Yield_ton_per_ha|    Concat_Region|
    +------+------+---------+-------+-----------+-------------+------------+------------------+----------+------------------+----------------+-------------+----------------+-----------------+
    | MAIZE|     C|    Sandy|   7.01|     1485.4|         19.7|        40.3|             105.1|      Drip|              10.2|            23.2|         Rice|          101.48| CODIGO_XXC-MAIZE|
    |BARLEY|     D|     Loam|   5.79|      399.4|         29.1|        55.4|             221.8| Sprinkler|              35.5|             7.4|       Barley|          127.39|CODIGO_XXD-BARLEY|
    |  RICE|     C|     Clay|   7.24|      980.9|         30.5|        74.4|              61.2| Sprinkler|              40.0|             5.1|        Wheat|           68.99|  CODIGO_XXC-RICE|
    | MAIZE|     D|     Loam|   6.79|     1054.3|         26.4|        62.0|             257.8|      Drip|              42.7|            23.7|         None|          169.06| CODIGO_XXD-MAIZE|
    | MAIZE|     D|    Sandy|   5.96|      744.6|         20.4|        70.9|             195.8|      Drip|              25.5|            15.6|        Maize|          118.71| CODIGO_XXD-MAIZE|
    |BARLEY|     C|    Sandy|   5.82|      817.5|         23.1|        47.6|              64.6|      None|              16.4|            16.2|        Maize|           58.85|CODIGO_XXC-BARLEY|
    |  RICE|     B|    Sandy|   6.76|     1358.2|         16.9|        31.9|             267.9| Sprinkler|              38.6|            23.6|         Rice|          173.44|  CODIGO_XXB-RICE|
    |  RICE|     D|    Sandy|    7.3|     1038.9|         34.1|        31.7|             269.4| Sprinkler|              16.0|            19.0|       Barley|          170.05|  CODIGO_XXD-RICE|
    | MAIZE|     C|     Loam|   6.94|      846.1|         32.4|        86.6|             263.2|      None|               7.4|            21.5|        Wheat|           162.2| CODIGO_XXC-MAIZE|
    +------+------+---------+-------+-----------+-------------+------------+------------------+----------+------------------+----------------+-------------+----------------+-----------------+
    only showing top 9 rows
    


#### 2: Transformación matemática

Los valores de lluvia tienen mucha varianza y el rendimiento tiene demasiados decimales irrelevantes. Añade/Modifica las siguientes columnas en df_eng:

Log_Rainfall: calcula el logaritmo natural (log) de la columna Rainfall_mm + 1 (para evitar errores si hubiera un 0).

Yield_Redondeado: redondea el rendimiento (Yield_ton_per_ha) a 1 solo decimal usando la función round.

Rendimiento_Bancario: crea otra columna usando bround sobre el rendimiento (sin decimales) para comparar cómo redondea Spark.


```python
from pyspark.sql.functions import log, round, bround

df = df \
    .withColumn("Rainfall",log(col("Rainfall_mm")) +1 ) \
    .withColumn("Yield_Redondeado",round(col("Yield_ton_per_ha"),1)) \
    .withColumn("Rendimiento_Bancario",bround(col("Yield_ton_per_ha"),0))

df.show(2)
```

    +------+--------+---------+-------+-----------+-------------+------------+------------------+----------+------------------+----------------+-------------+----------------+-----------------+----------------+--------------------+
    |  Crop|  Region|Soil_Type|Soil_pH|Rainfall_mm|Temperature_C|Humidity_pct|Fertilizer_Used_kg|Irrigation|Pesticides_Used_kg|Planting_Density|Previous_Crop|Yield_ton_per_ha|         Rainfall|Yield_Redondeado|Rendimiento_Bancario|
    +------+--------+---------+-------+-----------+-------------+------------+------------------+----------+------------------+----------------+-------------+----------------+-----------------+----------------+--------------------+
    | Maize|Region_C|    Sandy|   7.01|     1485.4|         19.7|        40.3|             105.1|      Drip|              10.2|            23.2|         Rice|          101.48|8.303439375235197|           101.5|               101.0|
    |Barley|Region_D|     Loam|   5.79|      399.4|         29.1|        55.4|             221.8| Sprinkler|              35.5|             7.4|       Barley|          127.39|6.989963420981715|           127.4|               127.0|
    +------+--------+---------+-------+-----------+-------------+------------+------------------+----------+------------------+----------------+-------------+----------------+-----------------+----------------+--------------------+
    only showing top 2 rows
    


#### 3.- Comparación de insumos

Queremos saber cuál fue el insumo químico más pesado aplicado en cada parcela. Crea una columna llamada Max_Quimico_kg.

Usa la función greatest para comparar, fila por fila, el valor de Fertilizer_Used_kg contra Pesticides_Used_kg. El resultado debe ser el valor más alto de los dos.




```python
from pyspark.sql.functions import greatest

df = df \
    .withColumn("Max_Quimico_kg", greatest(col("Fertilizer_Used_kg"),col("Pesticides_Used_kg")))

df.select(col("Max_Quimico_kg"),col("Fertilizer_Used_kg"),col("Pesticides_Used_kg")).show(4)

```

    +--------------+------------------+------------------+
    |Max_Quimico_kg|Fertilizer_Used_kg|Pesticides_Used_kg|
    +--------------+------------------+------------------+
    |         105.1|             105.1|              10.2|
    |         221.8|             221.8|              35.5|
    |          61.2|              61.2|              40.0|
    |         257.8|             257.8|              42.7|
    +--------------+------------------+------------------+
    only showing top 4 rows
    


#### 4.- Simulación de fechas

El dataset original no tiene fecha, pero sabemos que todos estos datos corresponden a la siembra del 1 de Abril de 2023.

Crea una columna Fecha_Siembra usando to_date sobre el literal “2023-04-01”.

Calcula la Fecha_Estimada_Cosecha sumando 150 días a la fecha de siembra (date_add).

Extrae el mes de la cosecha en una columna nueva llamada Mes_Cosecha (month).


```python
from pyspark.sql.functions import to_date,date_add,month
df = (df \
    .withColumn("Fecha_siembra",to_date(lit("2023-04-01"))) \
    .withColumn("Fecha_Estimada_Cosecha",date_add(col("Fecha_siembra"),150)) \
    .withColumn("Mes_Cosecha",month(col("Fecha_Estimada_Cosecha")))
)
df.select("Mes_Cosecha").show(1)
```

    +-----------+
    |Mes_Cosecha|
    +-----------+
    |          8|
    +-----------+
    only showing top 1 row
    

