------------- ESPECIALIZACIÓN EN INTELIGENCIA ARTIFICIAL Y BIG DATA -------------
---------------------------------------------------------------------------------


Módulo:                     BIG DATA APLICADO

Profesor:                   Víctor J. González

Unidad de Trabajo:          UT05. Procesamiento distribuido con PySpark

Práctica:                   PR0504A. Limpieza de datos sobre dataset de lugares famosos

Resultados de aprendizaje:  RA1

## PR0504. Limpieza de datos sobre dataset de lugares famosos

Seguimos trabajando con dataframes en PySpark. En esta ocasión el objetivo es transformar datos crudos de destinos turísticos para limpieza de texto, cálculos matemáticos avanzados y gestión de fechas.

Supón que en la empresa en la que estás trabajando está preparando un catálogo turístico y el departamento de marketing necesita un dataset enriquecido con códigos cortos para la app móvil, precios ajustados psicológicamente y fechas límite para ofertas promocionales.

Trabajarás sobre el archivo el mismo dataset de la práctica anterior.


```python
#Creamos la sesion y cogemos los datos
from pyspark.sql import SparkSession

try:
    spark = ( SparkSession.builder
                .appName("angel_spark")
                .master("spark://spark-master:7077")
                .getOrCreate()
            )
    print("SparkSession iniciada correctamente.")
except Exception as e:
    print("Error en la conexion")
    print(e)

```

    Setting default log level to "WARN".
    To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
    26/02/01 17:35:28 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable


    SparkSession iniciada correctamente.



```python
from pyspark.sql.types import StructType, StructField, StringType, DoubleType, BooleanType,IntegerType, LongType
schema1 = StructType([
    StructField("Place_Name",StringType(),True),
    StructField("Country",StringType(),True),
    StructField("City",StringType(),True),
    StructField("Annual_Visitors_Millions",IntegerType(),True),
    StructField("Type",StringType(),True),
    StructField("UNESCO_World_Heritage",StringType(),True),
    StructField("Year_Built",IntegerType(),True),
    StructField("Entry_Fee_USD",IntegerType(),True),
    StructField("Best_Visit_Month",StringType(),True),
    StructField("Region",StringType(),True),
    StructField("Tourism_Revenue_Million_USD",IntegerType(),True),
    StructField("Average_Visit_Duration_Hours",DoubleType(),True),
    StructField("Famous_For",StringType(),True)
])
df = (spark.read
        .format("csv")
        .option("header","true")
        .schema(schema1)
        .load("./world_famous_places_2024.csv")
     )

```

##### Ejercicio 1: Generación de códigos SKUs

La App móvil no puede mostrar nombres largos. Necesitamos un SKU (Stock Keeping Unit) para cada lugar.

Para ello tienes que crear una columna SKU_Lugar en un nuevo DataFrame df_feat.

El formato debe ser PAIS(3)-CIUDAD(3)-TIPO. Debes tener en cuenta:

    - País: extrae los 3 primeros caracteres del Country y conviértelos a mayúsculas (upper, substring).
    - Ciudad: extrae los 3 primeros caracteres de City. Si la ciudad tiene menos de 3 letras (raro, pero posible), rellena con ‘X’ a la derecha (rpad).
    - Tipo: la columna Type a veces tiene barras (ej: “Monument/Tower”). Queremos solo la primera parte antes de la barra. Usa split para dividir el texto y extrae el primer elemento (índice 0).
    - Unión: concatena todo con guiones bajos (concat_ws).


```python
df.show(1)

```

                                                                                    

    +------------+-------+-----+------------------------+--------------+---------------------+----------+-------------+-----------------+--------------+---------------------------+----------------------------+--------------------+
    |  Place_Name|Country| City|Annual_Visitors_Millions|          Type|UNESCO_World_Heritage|Year_Built|Entry_Fee_USD| Best_Visit_Month|        Region|Tourism_Revenue_Million_USD|Average_Visit_Duration_Hours|          Famous_For|
    +------------+-------+-----+------------------------+--------------+---------------------+----------+-------------+-----------------+--------------+---------------------------+----------------------------+--------------------+
    |Eiffel Tower| France|Paris|                       7|Monument/Tower|                   No|      1889|           35|May-June/Sept-Oct|Western Europe|                         95|                         2.5|Iconic iron latti...|
    +------------+-------+-----+------------------------+--------------+---------------------+----------+-------------+-----------------+--------------+---------------------------+----------------------------+--------------------+
    only showing top 1 row
    


    26/02/01 17:35:42 WARN GarbageCollectionMetrics: To enable non-built-in garbage collector(s) List(G1 Concurrent GC), users should configure it(them) to spark.eventLog.gcMetrics.youngGenerationGarbageCollectors or spark.eventLog.gcMetrics.oldGenerationGarbageCollectors



```python
from pyspark.sql.functions import col, substring,concat_ws,upper,rpad,split
df = (df
      .withColumn("SKU_Lugar",concat_ws("_",upper(substring(col("Country"),1,3)),rpad(upper(substring(col("City"),1,3)),3,"X"),split(col("Type"),"/")[0])) \

)
df.select("SKU_Lugar").show(3)
```

    +--------------------+
    |           SKU_Lugar|
    +--------------------+
    |    FRA_PAR_Monument|
    |UNI_NEW_Urban Lan...|
    |      FRA_PAR_Museum|
    +--------------------+
    only showing top 3 rows
    


##### Ejercicio 2: Ajuste de precios y tiempos

Necesitamos normalizar las métricas para el algoritmo de recomendación.

Añade las siguientes columnas numéricas:

    - Duracion_Techo: la Average_Visit_Duration_Hours tiene decimales (2.5 horas). Redondea siempre hacia arriba (ceil) para reservar bloques completos en la agenda del turista.
    - Log_Ingresos: los ingresos (Tourism_Revenue_Million_USD) varían demasiado (de 45 a 180). Aplica una transformación logarítmica (log10) para suavizar la escala.
    - Mejor_Oferta: compara el Entry_Fee_USD actual contra un “Precio de Competencia” simulado (que es siempre 20 USD). Usa la función least para quedarte con el precio más bajo de los dos (fila a fila).


```python
from pyspark.sql.functions import ceil,log10,least,lit
df = (df 
    .withColumn("Duracion_Techo",ceil(col("Average_Visit_Duration_Hours")))\
    .withColumn("Log_Ingresos",log10(col("Tourism_Revenue_Million_USD")))\
    .withColumn("Mejor_Oferta",least(col("Entry_Fee_USD"),lit(20)))
)
df.select("Duracion_Techo","Log_Ingresos","Mejor_Oferta").show(2)
```

    +--------------+------------------+------------+
    |Duracion_Techo|      Log_Ingresos|Mejor_Oferta|
    +--------------+------------------+------------+
    |             3|1.9777236052888478|          20|
    |             2| 1.845098040014257|           0|
    +--------------+------------------+------------+
    only showing top 2 rows
    



##### Ejercicio 3: Limpieza de texto

La columna Famous_For es demasiado larga para las notificaciones push.

Haz lo siguiente:

    - Crea Desc_Corta: extrae solo los primeros 15 caracteres de Famous_For (substring).
    - Crea Ciudad_Limpia: reemplaza la cadena “New York City” por “NYC” usando regexp_replace en la columna City.


```python
from pyspark.sql.functions import regexp_replace
df = (df
    .withColumn("Desc_Corta",substring(col("Famous_For"),1,15))\
    .withColumn("Ciudad_Limpia",regexp_replace(col("City"),lit("New York City"),lit("NYC")))
     
)
df.select("Desc_Corta","Ciudad_Limpia").show(2)
```

    +---------------+-------------+
    |     Desc_Corta|Ciudad_Limpia|
    +---------------+-------------+
    |Iconic iron lat|        Paris|
    |Bright lights, |          NYC|
    +---------------+-------------+
    only showing top 2 rows
    


##### Ejercicio 4: Gestión de fechas de campaña

Vamos a simular que lanzamos una campaña hoy.

    - Crea una columna Inicio_Campana usando to_date con la fecha “2024-06-01”.
    - Crea Fin_Campana: Suma 90 días a la fecha de inicio (date_add).
    - Crea Dias_Hasta_Fin: Calcula la diferencia en días entre el fin de la campaña y la fecha de construcción del monumento.

Nota:

Como Year_Built es un número (ej. 1889), primero deberás crear una fecha ficticia de construcción. Usa concat para unir el año con “-01-01” (ej: “1889-01-01”) y conviértelo a fecha con to_date.
Si el año tiene texto (como “220 BC”), to_date devolverá null, lo cual es correcto para este ejercicio.



```python
from pyspark.sql.functions import to_date,date_add,datediff,concat
df = (df
    .withColumn("Inicio_Campaña",to_date(lit("2024-06-01")))\
    .withColumn("Fin_Campaña",date_add(col("Inicio_Campaña"),90))\
    .withColumn("Dias_Hasta_fin",datediff(col("Fin_Campaña"),col("Fecha_Construccion")))
    .withColumn("Fecha_Construccion",to_date(concat(col("Year_Built"),lit("-01-01"))))
)
df.select("Inicio_Campaña","Fin_Campaña","Dias_Hasta_fin","Fecha_Construccion").show(2)
```

    +--------------+-----------+--------------+------------------+
    |Inicio_Campaña|Fin_Campaña|Dias_Hasta_fin|Fecha_Construccion|
    +--------------+-----------+--------------+------------------+
    |    2024-06-01| 2024-08-30|         49549|        1889-01-01|
    |    2024-06-01| 2024-08-30|         44072|        1904-01-01|
    +--------------+-----------+--------------+------------------+
    only showing top 2 rows
    


