# PR0201: Instalación y configuración de Hadoop en modo pseudo-distribuido

### ------------- ESPECIALIZACIÓN EN INTELIGENCIA ARTIFICIAL Y BIG DATA -------------
---------------------------------------------------------------------------------

Módulo:                     BIG DATA APLICADO
Profesor:                   Víctor J. González
Unidad de Trabajo:          UT02. HDFS. Almacenamiento distribuido
Práctica:                   PR0201. Instalación de Hadoop en modo pseudo-distribuido
Resultados de aprendizaje:  RA1, RA2 y RA3



Lo primero que hacemos es configurar la red y el nombre de la máquina.
Por la parte de la red tenemos que tener dos adaptadores una NAT  y otra solo anfitriona que en mi caso asigno la dirección **192.168.56.10**.

Por la parte del nombre, asignamos el nombre de la máquina como **Haddop-abr**, y como usuario creamos el usuario alumno con la contraseña "paso" 

![Configuración](./configuracion.png)
![Configuración2](./configuracion2.png)



Para el uso de hadoop, crearemos un nuevo usuario llamado **hadoop**

```bash
sudo adduser hadoop
```
![Captura0](./captura0.png)

Además le damos permiso para realizar las operaciones con **sudo**

```bash
sudo usermod -a -G sudo hadoop
```

![Captura1](./captura1.png)

Seguidamente, creamos la carpeta personal del nuevo usuario y le hacemos propietario.

```bash
cd /opt
sudo mkdir hadoop
sudo chown hadoop /opt/hadoop
```

![Captura2](./Captura2.png)

Comenzamos a usar el usuario creado para poder trabajar con él.

![Captura3](./Captura3.png)

Procederemos a la **Instalación** de apache **Hadoop**

![Captura4](./Captura4.png)

![Captura5](./Captura5.png)

Comprobamos que los hashes son iguales para confirmar que no se ha dañado ningun archivo durante la descarga

![Captura6](./captura6.png)

Una vez confirmado, descomprimimos el archivo

![captura7](./captura7.png)

Descargamos **jdk** y descomprimimos

![captura8](./captura8.png)

![captura9](./captura9.png)

![captura10](./captura10.png)

Para que sea más fácil de recordar, **renombremos** el archivo

![captura11](./captura11.png)

Comprobamos la **versión** instalada de Java

![captura12](./captura12.png)

Modificamos el archivo **bashrc**

![captura13](./captura13.png)

Recargamos el archivo 

![captura14](./captura14.png)

Comprobamos 

![captura15](./captura15.png)

Modificamos el siguiente archivo que modifica la configuración de hadoop

```bash
/otp/hadoop/etc/hadoop/hadoop-env.sh
```
![captura16](./captura16.png)

Añadiendo la siguiente línea

![captura17](./captura17.png)

Comprobamos mediante el uso de procesos en paralelo con **mapreduce**

![captura18](./captura18.png)

![captura19](./captura19.png)

Una vez comprobado, pasamos a instalar **SSH**

Para ello lo primero que hacemos es generar un par de claves con el siguiente comando:

```bash
ssh-keygen -b 2048
```
![captura20](./captura20.png)

Vamos al directorio personal y buscamos el directorio ssh, donde se crean las claves y añadimos la clave pública al archivo authorized keys

![captura21](./captura21.png)

Asignamos permisos y confirmamos

![captura22](./captura22.png)

Finalizada la configuración, probamos a conectarnos sin necesidad de contraseña

![captura23](./captura23.png)

Procedemos a configurar **HDFS**

Para ello necesitaremos modificar el siguiente archivo xml


```bash
/opt/hadoop/etc/hadoop/core-site.xml
```
Lo dejamos de la siguiente manera

![captura24](./captura24.png)

También será necesario modificar el siguiente archivo

```bash
/opt/hadoop/etc/hadoop/hdfs-site.xml
```
Lo dejamos de la siguiente manera

![captura25](./captura25.png)

Como hemos indicado en el archivo, tendremos que crear los siguientes directorios

```bash
/opt/hadoop/workspace/dfs/name
/opt/hadoop/workspace/dfs/data
```

Además tendremos que formatear el namenode, es decir inicializar el sistema de archivos distribuido HDFS antes de usarlo por primera vez
Para ello usamos

```bash
hdfs namenode -format
```
![captura26](./captura26.png)

Vemos el contenido del directorio

![captura27](./captura27.png)

![captura28](./captura28.png)

**Activamos Hadoop** (Se puede activar desde cualquier directorio)

![captura29](./captura29.png)

Comprobamos que se han puesto en marcha con el comando **JPS**

![captura30](./captura30.png)

Podemos comprobar también que funciona desde la interfaz web con la **ip: puerto9870**

![captura31](./captura31.png)

**Comandos Básicos de CLI de HDFS**

Toda la interacción con HDFS se hace mediante el comando hdfs + subcomandos
Los comandos de clientes utilizan el subcomando dfs
(Utiliza por comodidad comandos de linux)
Saber que comandos puedo usar

```bash
Hdfs dfs -help
```

Para trabajar con ficheros 

```bash
Hdfs dfs
```
Ejemplo de comandos

![captura32](./captura32.png)

Para crear una carpeta

![captura33](./captura33.png)

Pasar un archivo de local a HDFS(Comando **put** ruta del archivo, ruta de hdfs donde se guardará)

![captura34](./captura34.png)

Comprobamos y miramos información desde la interfaz web

![captura35](./captura35.png)


Copia de archivo de gran tamaño

![captura36](./captura36.png)

Miramos la información de este archivo(Como tiene más tamaño este está dividido en más bloques)

![captura37](./captura37.png)

Podemos ver el espacio que ocupa

![captura38](./captura38.png)

Para **borrar** el archivo, tendremos que borrarlo tanto en el local como en el hdfs

![captura39](./captura39.png)